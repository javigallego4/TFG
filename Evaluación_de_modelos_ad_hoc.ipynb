{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javigallego4/TFG/blob/main/Evaluaci%C3%B3n_de_modelos_ad_hoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftn3Nk2Sny7c",
        "outputId": "5477c379-ab07-4ae4-b37b-9c9b772dd45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPUs:  4\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from IPython.display import clear_output, display_html\n",
        "import gc; gc.enable()\n",
        "import warnings\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Basic libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy as sc\n",
        "from scipy import stats\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# Preprocessing libraries\n",
        "from sklearn.preprocessing import *\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "# Library for .tiff files\n",
        "!pip install tifffile\n",
        "import tifffile as tiff\n",
        "\n",
        "# Timm Library\n",
        "!pip install timm\n",
        "import timm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "\n",
        "# MaskRCNN class imports\n",
        "from typing import Any, Callable, Optional\n",
        "from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor\n",
        "from torch import nn\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision.ops import misc as misc_nn_ops\n",
        "from torchvision.transforms._presets import ObjectDetection\n",
        "\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNN, MaskRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models import *\n",
        "from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor\n",
        "\n",
        "# Deep Lab V3 Backbones\n",
        "from torchvision.models.segmentation.deeplabv3 import *\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101, deeplabv3_mobilenet_v3_large, DeepLabV3_ResNet101_Weights\n",
        "\n",
        "# Metric (mAP)\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Weights and biases\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "# Memory usage\n",
        "import gc\n",
        "def gc_collect():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = '5bf911e7e682da23240c68fb146a222bf0475f7c'\n",
        "wandb.login() # 5bf911e7e682da23240c68fb146a222bf0475f7c\n",
        "\n",
        "clear_output()\n",
        "print('Number of CPUs: ', os.cpu_count())\n",
        "\n",
        "DEBUG = False\n",
        "LOG_IMAGES = False\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "train_validation_test_split = pd.read_csv('/content/gdrive/MyDrive/train_validation_test_split.csv', index_col = False)\n",
        "\n",
        "\"\"\"Nos creamos los índices para los conjuntos de entrenamiento, validación y test.\"\"\"\n",
        "\n",
        "X_train = train_validation_test_split[train_validation_test_split['set'] == 'train'].image.values\n",
        "X_test = train_validation_test_split[train_validation_test_split['set'] == 'test'].image.values\n",
        "X_val = train_validation_test_split[train_validation_test_split['set'] == 'val'].image.values\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "\n",
        "# Obtenemos todos los nºs de polígonos que nos dan en la siguiente ruta.\n",
        "\n",
        "BASE_PATH = \"/content/gdrive/MyDrive/juniper/WV3/\"\n",
        "polygon_numbers = os.listdir(BASE_PATH)\n",
        "polygon_numbers = pd.Series(polygon_numbers).str.split('_', n = 2, expand = True)[1]\n",
        "polygon_numbers = list(polygon_numbers)\n",
        "polygon_numbers = sorted(polygon_numbers)\n",
        "\n",
        "# Guardamos en arrays cada una de las imágenes y máscaras.\n",
        "\n",
        "def load_images(polygon_numbers):\n",
        "  for polygon_number in polygon_numbers:\n",
        "      # Panchromatic Images\n",
        "      p_images.append(tiff.imread(BASE_PATH + \"polygon_{}/panchromatic.tif\".format(polygon_number)))\n",
        "      p_masks.append(tiff.imread(BASE_PATH + \"polygon_{}/mask_panchromatic.tif\".format(polygon_number)))\n",
        "\n",
        "      # Multispectral Images\n",
        "      # Hacemos un permute para poner las imágenes en el formato PyTorch\n",
        "      m_images.append(tiff.imread(BASE_PATH + \"polygon_{}/multispectral.tif\".format(polygon_number)))\n",
        "      m_masks.append(tiff.imread(BASE_PATH + \"polygon_{}/mask_multispectral.tif\".format(polygon_number)))\n",
        "\n",
        "\"\"\"## Pansharpening\"\"\"\n",
        "\n",
        "def histogram_match(pan, band):\n",
        "    \"\"\"\n",
        "    Performs histogram matching between the panchromatic image and the multispectral band given.\n",
        "\n",
        "    Parameters:\n",
        "    - pan: torch tensor of shape (height, width)\n",
        "    - band: torch tensor of shape (height, width)\n",
        "\n",
        "    Returns:\n",
        "    - matched_panchromatic: histogram matched PAN imagery\n",
        "    \"\"\"\n",
        "\n",
        "    # Fórmula UGR: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwicxYCXrYv9AhXRhqQKHYoUDFQQFnoECAkQAQ&url=https%3A%2F%2Fccia.ugr.es%2Fvip%2Ffiles%2Fbooks%2Fpaper_amro_mateos.pdf&usg=AOvVaw3wn01QiErGCJLtNZUg-oKe\n",
        "    matched_panchromatic = (pan - pan.mean()) * (band.std() / pan.std()) + band.mean()\n",
        "\n",
        "    return matched_panchromatic\n",
        "\n",
        "def pansharpen_image(multispectral_image, panchromatic_image, method):\n",
        "    \"\"\"\n",
        "    Pansharpens the given MS image based on different techniques.\n",
        "\n",
        "    Parameters:\n",
        "    - multispectral_image: torch tensor of shape (channels, height, width)\n",
        "    - panchromatic_image: torch tensor of shape (height, width)\n",
        "    - method: type of pansharpening technique\n",
        "\n",
        "    Returns:\n",
        "    - sharpened_img: torch tensor with same shape as input multispectral image\n",
        "    \"\"\"\n",
        "\n",
        "    if method == 'Simple Mean':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])\n",
        "      for i in range(multispectral_image.shape[0]):\n",
        "        # Histogram Matching for each band\n",
        "        matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])\n",
        "        sharpened_img[i, :, :] = 0.5 * (multispectral_image[i, :, :] + matched_panchromatic)\n",
        "\n",
        "    if method == 'Brovey':\n",
        "        multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "        sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])\n",
        "\n",
        "        M = 0\n",
        "        for i in range(multispectral_image.shape[0]):\n",
        "          M += multispectral_image[i, :, :]\n",
        "        M /= multispectral_image.shape[0]\n",
        "\n",
        "        for i in range(multispectral_image.shape[0]):\n",
        "          # Histogram Matching for each band\n",
        "          matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])\n",
        "          sharpened_img[i, :, :] = (matched_panchromatic / M) * multispectral_image[i, :, :]\n",
        "\n",
        "    if method == 'HSV':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 0.577 * (red+green+blue)\n",
        "      v1 = -0.408 * (red+green) + 0.816 * blue\n",
        "      v2 = -0.707 * (red+green) + 1.703 * blue\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 0.577 * matched_panchromatic - 0.408 * v1 - 0.707 * v2\n",
        "      new_green = 0.577 * matched_panchromatic - 0.408 * v1 - 0.816 * v2\n",
        "      new_blue = 0.577 * matched_panchromatic - 0.816 * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS1':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/np.sqrt(3) * (red+green+blue)\n",
        "      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(2) * (-red+green)\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      v1 = S * torch.cos(H)\n",
        "      v2 = S * torch.sin(H)\n",
        "\n",
        "      new_red = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 - 1/np.sqrt(2) * v2\n",
        "      new_green = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2\n",
        "      new_blue = 1/np.sqrt(3) * matched_panchromatic + 2/np.sqrt(6) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS2':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(6) * red - 2 /np.sqrt(6) * green\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      v1 = S * torch.cos(2*np.pi*H)\n",
        "      v2 = S * torch.sin(2*np.pi*H)\n",
        "\n",
        "      new_red = 1 * matched_panchromatic -0.204124 * v1 - 0.612372 * v2\n",
        "      new_green = 1 * matched_panchromatic -0.204124 * v1 + 0.612372 * v2\n",
        "      new_blue = 1 * matched_panchromatic + 0.408248 * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS3':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(6) * red - 1/np.sqrt(6) * green\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 1 * matched_panchromatic -1/np.sqrt(6) * v1 +3/np.sqrt(6) * v2\n",
        "      new_green = 1 * matched_panchromatic -1/np.sqrt(6) * v1 -3/np.sqrt(6) * v2\n",
        "      new_blue = 1 * matched_panchromatic + 2/np.sqrt(6) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS4':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green\n",
        "\n",
        "      H = torch.atan(v1/v2)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2\n",
        "      new_green = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 -1/np.sqrt(2) * v2\n",
        "      new_blue = 1/3 * matched_panchromatic - 1/np.sqrt(2) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS5':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2\n",
        "      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2\n",
        "      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'IHS6':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = -2/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green\n",
        "\n",
        "      H = torch.atan(v2/v1)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 1 * matched_panchromatic -1/np.sqrt(2) * v1 + 1/np.sqrt(2) * v2\n",
        "      new_green = 1 * matched_panchromatic -1/np.sqrt(2) * v1  -1/np.sqrt(2) * v2\n",
        "      new_blue = 1 * matched_panchromatic + np.sqrt(2) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    if method == 'HLS':\n",
        "      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)\n",
        "      # Forward Transform\n",
        "      red = multispectral_image[4, :, :]\n",
        "      green = multispectral_image[2, :, :]\n",
        "      blue = multispectral_image[1, :, :]\n",
        "\n",
        "      I = 1/3 * (red+green+blue)\n",
        "      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue\n",
        "      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green\n",
        "\n",
        "      H = torch.atan(v1/v2)\n",
        "      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))\n",
        "\n",
        "      # Histogram Matching\n",
        "      matched_panchromatic = histogram_match(panchromatic_image, I)\n",
        "\n",
        "      # Reverse Transformation\n",
        "      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2\n",
        "      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2\n",
        "      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1\n",
        "\n",
        "      sharpened_img = multispectral_image\n",
        "      sharpened_img[4, :, :] = new_red\n",
        "      sharpened_img[2, :, :] = new_green\n",
        "      sharpened_img[1, :, :] = new_blue\n",
        "\n",
        "    return sharpened_img\n",
        "\n",
        "def pansharpening(method):\n",
        "  \"\"\"\n",
        "  Performs pansharpening to every image in the dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(p_images)):\n",
        "    img = pansharpen_image(torch.from_numpy(m_images[i]).permute(2,0,1), torch.from_numpy(p_images[i]), method)\n",
        "    m_images[i] = img.permute(1,2,0).numpy()\n",
        "\n",
        "\"\"\"## Scaling\"\"\"\n",
        "\n",
        "def scale_panchromatic_image(image, transformer = MinMaxScaler()):\n",
        "  '''Returns input panchromatic image with its values being scaled to the [0,1] interval. '''\n",
        "\n",
        "  img = transformer.fit_transform(image, )\n",
        "  return img\n",
        "\n",
        "def scale_multispectral_image(image, bands = 8, transformer = MinMaxScaler()):\n",
        "  '''Returns input multispectral image with its values being scaled to the [0,1] interval. '''\n",
        "\n",
        "  b0 = image[:,:,0]\n",
        "  b1 = image[:,:,1]\n",
        "  b2 = image[:,:,2]\n",
        "  if bands == 8:\n",
        "    b3 = image[:,:,3]\n",
        "    b4 = image[:,:,4]\n",
        "    b5 = image[:,:,5]\n",
        "    b6 = image[:,:,6]\n",
        "    b7 = image[:,:,7]\n",
        "\n",
        "  # As before, we perform some scaling first\n",
        "  sc = transformer\n",
        "  b0 = sc.fit_transform(b0)\n",
        "  b1 = sc.fit_transform(b1)\n",
        "  b2 = sc.fit_transform(b2)\n",
        "  if bands == 8:\n",
        "    b3 = sc.fit_transform(b3)\n",
        "    b4 = sc.fit_transform(b4)\n",
        "    b5 = sc.fit_transform(b5)\n",
        "    b6 = sc.fit_transform(b6)\n",
        "    b7 = sc.fit_transform(b7)\n",
        "\n",
        "  if bands == 8: img = np.dstack([b0, b1, b2, b3, b4, b5, b6, b7])\n",
        "  else: img = np.dstack([b0, b1, b2])\n",
        "  return img\n",
        "\n",
        "def scaling():\n",
        "  '''Pipeline function for value scaling. '''\n",
        "\n",
        "  for i in range(len(p_images)):\n",
        "    p_images[i] = scale_panchromatic_image(p_images[i])\n",
        "    m_images[i] = scale_multispectral_image(m_images[i])\n",
        "\n",
        "\"\"\"## Data Augmentation\"\"\"\n",
        "\n",
        "def HorizontalFlip():\n",
        "  '''Performs horizontal flipping. '''\n",
        "\n",
        "  for i in range(len(polygon_numbers)):\n",
        "    m_images.append(cv2.flip(m_images[i], 1))\n",
        "    p_masks.append(cv2.flip(p_masks[i], 1))\n",
        "\n",
        "def VerticalFlip():\n",
        "  '''Performs vertical flipping. '''\n",
        "\n",
        "  for i in range(len(polygon_numbers)):\n",
        "    m_images.append(cv2.flip(m_images[i], 0))\n",
        "    p_masks.append(cv2.flip(p_masks[i], 0))\n",
        "\n",
        "def VHFlip():\n",
        "  '''Performs both horizontal and vertical flipping. '''\n",
        "\n",
        "  for i in range(len(polygon_numbers)):\n",
        "    m_images.append(cv2.flip(m_images[i], -1))\n",
        "    p_masks.append(cv2.flip(p_masks[i], -1))\n",
        "\n",
        "def Rotation90():\n",
        "  '''Performs a 90 degrees rotation on the images'''\n",
        "\n",
        "  for i in range(len(polygon_numbers)):\n",
        "\n",
        "    # Transpose the image\n",
        "    image = image.transpose(1,0,2)\n",
        "    # Flip the image vertically\n",
        "    image = cv2.flip(image, 1)\n",
        "    m_images.append(cv2.flip(m_images[i], -1))\n",
        "    p_masks.append(cv2.flip(p_masks[i], -1))\n",
        "\n",
        "# source: https://www.kaggle.com/safavieh/image-augmentation-using-skimage\n",
        "import random\n",
        "import pylab as pl\n",
        "\n",
        "def random_crop(img, mask, crop_height, crop_width):\n",
        "\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "\n",
        "    # Calculate aspect ratio\n",
        "    aspect_ratio = float(width / height)\n",
        "\n",
        "    # Calculate the maximum width and height that can be cropped while maintaining the aspect ratio\n",
        "    max_crop_width = int(aspect_ratio * crop_height)\n",
        "    max_crop_height = int(crop_width / aspect_ratio)\n",
        "\n",
        "    # Choose a random starting point for the crop\n",
        "    start = random.randint(0,10)\n",
        "    x = random.randint(start, width - max_crop_width)\n",
        "    y = random.randint(start, height - max_crop_height)\n",
        "\n",
        "    # Crop the image and mask\n",
        "    cropped_img = img[y:y+max_crop_height, x:x+max_crop_width]\n",
        "    cropped_mask = mask[y:y+max_crop_height, x:x+max_crop_width]\n",
        "\n",
        "    # Resize the cropped image and mask to the desired size\n",
        "    # Interpolation. Possible values: cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_AREA, cv2.INTER_CUBIC\n",
        "    resized_image = cv2.resize(cropped_img, (width, height), interpolation=cv2.INTER_LINEAR)\n",
        "    resized_mask = cv2.resize(cropped_mask, (width, height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return resized_image, resized_mask\n",
        "\n",
        "def RandomCropping():\n",
        "    ''' Applying 10 random croppings to all the images.'''\n",
        "\n",
        "    for i in range(len(polygon_numbers)):\n",
        "      for j in range(10):\n",
        "          width = random.randint(40, 60)\n",
        "          aspect_ratio = m_images[i].shape[0] / m_images[i].shape[1]\n",
        "          img, mask = random_crop(m_images[i], p_masks[i], int(width*aspect_ratio), width)\n",
        "          m_images.append(img)\n",
        "          p_masks.append(mask)\n",
        "\n",
        "def data_augmentation():\n",
        "  '''Performs data augmentation over images and masks. '''\n",
        "\n",
        "  HorizontalFlip()\n",
        "  VerticalFlip()\n",
        "  VHFlip()\n",
        "  RandomCropping()\n",
        "\n",
        "\"\"\"## Padding\"\"\"\n",
        "\n",
        "def pad_images(imgs, msks, border):\n",
        "  '''Pipeline function for images' padding. '''\n",
        "\n",
        "  border_type = border\n",
        "  images, masks = [], []\n",
        "  for i in range(len(imgs)):\n",
        "    images.append(cv2.copyMakeBorder(imgs[i], 128 - imgs[i].shape[0], 0, 80 - imgs[i].shape[1], 0, border_type))\n",
        "    masks.append(cv2.copyMakeBorder(msks[i], 128 - msks[i].shape[0], 0, 80 - msks[i].shape[1], 0, border_type))\n",
        "\n",
        "  return images, masks\n",
        "\n",
        "\"\"\"## Preprocessing Pipeline Main Function\"\"\"\n",
        "\n",
        "def preprocessing_pipeline(method, border_type = cv2.BORDER_CONSTANT, scale = True):\n",
        "  for i in range(len(m_images)):\n",
        "    m_images[i] = m_images[i].astype(float)\n",
        "    p_images[i] = p_images[i].astype(float)\n",
        "\n",
        "  if scale: scaling()\n",
        "  pansharpening(method)\n",
        "  data_augmentation()\n",
        "  imgs, masks = pad_images(m_images, p_masks, border_type)\n",
        "\n",
        "  for i in range(len(imgs)):\n",
        "    masks[i] = masks[i].astype(int)\n",
        "\n",
        "  return imgs, masks\n",
        "\n",
        "\"\"\"# PyTorch Dataset\"\"\"\n",
        "\n",
        "class PanchromaticDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, masks):\n",
        "      super().__init__()\n",
        "      self.images = images\n",
        "      self.masks = masks\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return torch.from_numpy(self.images[i].astype(float)), torch.from_numpy(self.masks[i].astype(float))\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "class MultispectralDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images, masks):\n",
        "    super().__init__()\n",
        "    self.images = images\n",
        "    self.masks = masks\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    img, mask = torch.from_numpy(self.images[i].astype(float)).permute(2,0,1), torch.from_numpy(self.masks[i].astype(float))\n",
        "    return torch.nan_to_num(img), torch.nan_to_num(mask)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "\"\"\"# Model\"\"\"\n",
        "\n",
        "\"\"\"### Model Implementation\"\"\"\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, maskrcnn_resnet50_fpn_v2\n",
        "\n",
        "\"\"\"# Model\n",
        "\n",
        "### Model Implementation\n",
        "\"\"\"\n",
        "\n",
        "from torchvision.models import *\n",
        "from torch.nn.modules import batchnorm\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    ''' Basic block for performing two convolution operations. '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, pad = 1, activation_function = nn.ReLU(inplace=True)):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride = 1, padding=pad)\n",
        "        batch1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride = 1, padding=pad)\n",
        "        batch2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv_block = nn.Sequential(conv1, batch1, activation_function, conv2, batch2, activation_function)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "class unetDown(nn.Module):\n",
        "    ''' Encoder block of the U-Net architecture. '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, pad = 1):\n",
        "        super(unetDown, self).__init__()\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels, pad)\n",
        "        self.pooling = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    def forward(self, x, indices = None):\n",
        "        skip_connection = self.conv_block(x)\n",
        "        x = self.pooling(skip_connection)\n",
        "        return x, skip_connection\n",
        "\n",
        "class unetUp(nn.Module):\n",
        "    ''' Decoder block of the U-Net architecture. '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, pad=1):\n",
        "        super(unetUp, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels, pad)\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upsample(x)\n",
        "        #print('nnnn')\n",
        "        #print(x.shape)\n",
        "        #print(skip_connection.shape)\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "class multispectralUnet(nn.Module):\n",
        "    ''' Full architecture of the proposed network. '''\n",
        "\n",
        "    def __init__(self, n_filters_1, n_filters_2, n_filters_3, n_filters_4):\n",
        "        super(multispectralUnet, self).__init__()\n",
        "\n",
        "        # ====== Encoder ======\n",
        "\n",
        "        self.down1 = unetDown(8, n_filters_1)\n",
        "        self.down2 = unetDown(n_filters_1, n_filters_2)\n",
        "        self.down3 = unetDown(n_filters_2, n_filters_3)\n",
        "\n",
        "        # ====== BottleNeck =====\n",
        "        self.bottleneck = ConvBlock(n_filters_3, n_filters_4)\n",
        "\n",
        "        # ===== Decoder =====\n",
        "\n",
        "        self.up3 = unetUp(n_filters_3 + n_filters_4, n_filters_3)\n",
        "        self.up2 = unetUp(n_filters_2 + n_filters_3, n_filters_2)\n",
        "        self.up1 = unetUp(n_filters_1 + n_filters_2, n_filters_1)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.conv_last = nn.Sequential(nn.Conv2d(n_filters_1, 1, kernel_size=1), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1 = self.down1(x)\n",
        "        x, skip2 = self.down2(x)\n",
        "        x, skip3 = self.down3(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        x = self.up3(x, skip3)\n",
        "        x = self.up2(x, skip2)\n",
        "        x = self.up1(x, skip1)\n",
        "        x = self.conv_last(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self,x):\n",
        "        x = self.forward(x)\n",
        "        return x\n",
        "\n",
        "\"\"\"# Training Pipeline\n",
        "\n",
        "### Helper Functions\n",
        "\"\"\"\n",
        "\n",
        "from torchmetrics import F1Score, Precision, Recall\n",
        "\n",
        "def optimal_f1(predictions, targets):\n",
        "    '''With this function we obtain the optimal threshold, given our model predictions. '''\n",
        "    thres = np.linspace(torch.min(predictions).item(), torch.max(predictions).item(), 201)\n",
        "    f1_score = F1Score(task=\"binary\", num_classes=2).to(device)\n",
        "    f1s = torch.Tensor([f1_score(predictions > thr, targets) for thr in thres])\n",
        "    idx = torch.argmax(f1s)\n",
        "    return f1s[idx], thres[idx]\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n",
        "def validate_one_epoch(validation_loader, model, thr):\n",
        "    #metric = MeanAveragePrecision(iou_type = 'bbox')\n",
        "    loss_epoch = 0\n",
        "    bce_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "    f1_epoch, precision_epoch, recall_epoch = 0.0, 0.0, 0.0\n",
        "    f1_score = F1Score(task=\"binary\", num_classes=2).to(device)\n",
        "    precision_score = Precision(task=\"binary\", num_classes=2).to(device)\n",
        "    recall_score = Recall(task=\"binary\", num_classes=2).to(device)\n",
        "\n",
        "    # Don't update weights\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Loop over minibatches\n",
        "      for imgs, masks in tqdm(validation_loader):\n",
        "          model.train()\n",
        "          imgs = imgs.to(device, dtype=torch.float32)\n",
        "          masks = masks.to(device)\n",
        "\n",
        "          # Make predictions and obtain losses\n",
        "          with torch.cuda.amp.autocast():\n",
        "            outputs = model(imgs)\n",
        "            loss = bce_loss(outputs.squeeze()[:, 29:, :], (masks[:, 29:, :]).to(device))\n",
        "\n",
        "          # Track losses\n",
        "          loss_epoch += loss.detach().item()\n",
        "\n",
        "          # Track metric\n",
        "          model.eval()\n",
        "          preds = model.predict(imgs).squeeze()\n",
        "\n",
        "          f1_epoch += f1_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :])\n",
        "          recall_epoch += recall_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :])\n",
        "          precision_epoch += precision_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :])\n",
        "\n",
        "    return loss_epoch/len(validation_loader), f1_epoch/len(validation_loader), precision_epoch/len(validation_loader), recall_epoch/len(validation_loader)\n",
        "\n",
        "def show_predictions(loader, model, thr):\n",
        "  # Ejemplo de la máscara predicha\n",
        "  # Don't update weights\n",
        "    with torch.no_grad():\n",
        "      for imgs, masks in tqdm(loader):\n",
        "        model.eval()\n",
        "\n",
        "        # Create MaskRCNN inputs and targets\n",
        "        inputs = [imgs[i].to(device, dtype = torch.float32) for i in range(imgs.shape[0])]\n",
        "\n",
        "        # Track metric\n",
        "        pred = model.predict(inputs)\n",
        "\n",
        "        for i in range(imgs.shape[0]):\n",
        "          predicted_mask = torch.sum(pred[i]['masks'], dim=0)\n",
        "\n",
        "          fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
        "          axes[0].imshow(masks[i])\n",
        "          axes[1].imshow((predicted_mask > thr).cpu().squeeze())\n",
        "          plt.show()\n",
        "\n",
        "        del inputs, pred\n",
        "        gc_collect()\n",
        "\n",
        "def validate_model(verbose=True):\n",
        "    torch.manual_seed(42)\n",
        "    # Init W&B\n",
        "    run = wandb.init(entity=\"javigallego4\", project=\"Bachelor Thesis\", group = 'Baseline - Ensembling')\n",
        "\n",
        "    # Preprocessing Pipeline (No initial scaling)\n",
        "    m_images2, p_masks2 = preprocessing_pipeline('Simple Mean', cv2.BORDER_REFLECT_101, False)\n",
        "\n",
        "    # Datasets and train-test-split\n",
        "    multispectral_dataset = MultispectralDataset(m_images2, p_masks2, 'Vegetation Indexes')\n",
        "    n = len(multispectral_dataset)\n",
        "    training_data = torch.utils.data.Subset(multispectral_dataset, X_train)\n",
        "    validation_data = torch.utils.data.Subset(multispectral_dataset, X_val)\n",
        "    test_data = torch.utils.data.Subset(multispectral_dataset, X_test)\n",
        "    gc_collect()\n",
        "\n",
        "    print('Dataset created')\n",
        "\n",
        "    # Load artifact\n",
        "    run_name = 'young-sweep-49'\n",
        "    artifact = run.use_artifact('javigallego4/Bachelor Thesis/{}:v0'.format(run_name), type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    # Load the model\n",
        "    model = maskrcnn_model('resnext101_32x8d').to(device)\n",
        "    model.load_state_dict(torch.load('/content/artifacts/{}:v0/{}.pt'.format(run_name, run_name)))\n",
        "    best_thr = 0.589\n",
        "\n",
        "    print('Model loaded')\n",
        "\n",
        "    batch = 4\n",
        "    trainloader = torch.utils.data.DataLoader(training_data, batch_size = batch, num_workers = 2, shuffle = True, pin_memory=True)\n",
        "    validationloader = torch.utils.data.DataLoader(validation_data, batch_size = batch, num_workers = 2, pin_memory=True)\n",
        "    testloader = torch.utils.data.DataLoader(test_data, batch_size = batch, num_workers = 2, pin_memory=True)\n",
        "\n",
        "    print('Preprocessing done')\n",
        "\n",
        "    # Evaluate\n",
        "    #val_f1, val_precision, val_recall = get_validation_scores(validationloader, model, best_thr)\n",
        "    #test_f1, test_precision, test_recall = get_validation_scores(testloader, model, best_thr)\n",
        "\n",
        "    # Print loss\n",
        "    #print(f'val_f1 {val_f1:.5f}, val_precision {val_precision:.5f}, val_recall {val_recall:.5f}')\n",
        "    #print(f'test_f1 {test_f1:.5f}, test_precision {test_precision:.5f}, test_recall {test_recall:.5f}')\n",
        "\n",
        "    show_predictions(testloader, model, best_thr)\n",
        "\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uomP87ZmudEj"
      },
      "outputs": [],
      "source": [
        "#p_images, m_images, p_masks, m_masks = [], [], [], []\n",
        "#fit_all_images(polygon_numbers)\n",
        "#validate_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1nqDI77Bn-fl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DmB98SeYn_qY"
      },
      "outputs": [],
      "source": [
        "def get_validation_masks(validation_loader, model):\n",
        "    masks_list = []\n",
        "    with torch.no_grad():\n",
        "      # Loop over minibatches\n",
        "      for imgs, masks in tqdm(validation_loader):\n",
        "          model.eval()\n",
        "\n",
        "          # Create MaskRCNN inputs and targets\n",
        "          inputs = imgs.to(device, dtype=torch.float32)\n",
        "          pred = model.predict(inputs)\n",
        "\n",
        "          for i in range(inputs.shape[0]):\n",
        "            masks_list.append(pred[i, :, 29:, :].squeeze())\n",
        "\n",
        "          del inputs, pred\n",
        "          gc_collect()\n",
        "\n",
        "    masks_pred = torch.stack(masks_list, 0)\n",
        "    return masks_pred\n",
        "\n",
        "def get_ensembling_scores(verbose=True):\n",
        "    torch.manual_seed(42)\n",
        "    # Init W&B\n",
        "    run = wandb.init(entity=\"javigallego4\", project=\"Bachelor Thesis\", group = 'Baseline - Ensembling')\n",
        "\n",
        "    # Preprocessing Pipeline (No initial scaling)\n",
        "    m_images2, p_masks2 = preprocessing_pipeline('Simple Mean', cv2.BORDER_REFLECT_101, False)\n",
        "\n",
        "    # Datasets and train-test-split\n",
        "    multispectral_dataset = MultispectralDataset(m_images2, p_masks2, 'Vegetation Indexes')\n",
        "    n = len(multispectral_dataset)\n",
        "    training_data = torch.utils.data.Subset(multispectral_dataset, X_train)\n",
        "    validation_data = torch.utils.data.Subset(multispectral_dataset, X_val)\n",
        "    test_data = torch.utils.data.Subset(multispectral_dataset, X_test)\n",
        "    gc_collect()\n",
        "\n",
        "    print('Dataset created')\n",
        "\n",
        "    batch = 16\n",
        "    trainloader = torch.utils.data.DataLoader(training_data, batch_size = batch, num_workers = 2, shuffle = True, pin_memory=True)\n",
        "    validationloader = torch.utils.data.DataLoader(validation_data, batch_size = batch, num_workers = 2, pin_memory=True)\n",
        "    testloader = torch.utils.data.DataLoader(test_data, batch_size = batch, num_workers = 2, pin_memory=True)\n",
        "\n",
        "    print('Preprocessing done')\n",
        "\n",
        "    # Load artifact\n",
        "    masks_list = []\n",
        "    best_runs = ['young-sweep-49','eternal-sweep-52','grateful-sweep-43']\n",
        "    backbones = ['resnext101_32x8d','resnet101','resnext101_32x8d']\n",
        "    thresholds = [0.589, 0.5856, 0.5737]\n",
        "    models = []\n",
        "    #weights = []\n",
        "    for i, run_name in enumerate(best_runs):\n",
        "      # Load artifact\n",
        "      artifact = run.use_artifact('javigallego4/Bachelor Thesis/{}:v0'.format(run_name), type='model')\n",
        "      artifact_dir = artifact.download()\n",
        "\n",
        "      # Load the model\n",
        "      model = maskrcnn_model(backbones[i]).to(device)\n",
        "      model.load_state_dict(torch.load('/content/artifacts/{}:v0/{}.pt'.format(run_name, run_name)))\n",
        "      models.append(model)\n",
        "\n",
        "      # Evaluate\n",
        "      #model, thresholds = models[best_runs[i]]\n",
        "      masks_list.append(get_validation_masks(validationloader, model))\n",
        "\n",
        "      del model, artifact\n",
        "      gc_collect()\n",
        "\n",
        "    ground_truth_masks = []\n",
        "    validationloader = torch.utils.data.DataLoader(validation_data, batch_size = 1, num_workers = 2, pin_memory=True)\n",
        "    for imgs, masks in tqdm(validationloader):\n",
        "      ground_truth_masks.append(masks.squeeze().to(device))\n",
        "\n",
        "    stacked_masks = torch.stack(masks_list)\n",
        "    for i in range(100):\n",
        "      fig, axes = plt.subplots(nrows = 1, ncols = 5, figsize=(20,4))\n",
        "      axes[0].imshow(stacked_masks[0][i].cpu())\n",
        "      axes[0].set_title('Modelo 1')\n",
        "      axes[1].imshow(stacked_masks[1][i].cpu())\n",
        "      axes[1].set_title('Modelo 2')\n",
        "      axes[2].imshow(stacked_masks[2][i].cpu())\n",
        "      axes[2].set_title('Modelo 3')\n",
        "      axes[4].imshow(ground_truth_masks[i].cpu())\n",
        "      axes[4].set_title('Mascara original')\n",
        "      plt.show()\n",
        "\n",
        "    print(stacked_masks.shape)\n",
        "    for w3 in np.arange(0.01, 1.0, 0.01):\n",
        "      for w2 in np.arange(0.01, 1.0, 0.01):\n",
        "        for w1 in np.arange(0.01, 1.0, 0.01):\n",
        "            weights = [w1, w2, w3]\n",
        "            tensor_weights = torch.tensor(weights)\n",
        "            expanded_weights = tensor_weights[:, None, None, None].to(device)\n",
        "            print('Pesos shape ', expanded_weights.shape)\n",
        "            weighted_sum = torch.sum(stacked_masks * expanded_weights, dim=0)\n",
        "            final_masks = weighted_sum / torch.sum(tensor_weights)\n",
        "            print(final_masks.shape)\n",
        "\n",
        "            #ground_truth_masks = torch.stack(masks_list, dim = 0)\n",
        "            #print(ground_truth_masks.shape)\n",
        "\n",
        "            f1_score = F1Score(task=\"binary\", num_classes=2).to(device)\n",
        "            precision_score = Precision(task=\"binary\", num_classes=2).to(device)\n",
        "            recall_score = Recall(task=\"binary\", num_classes=2).to(device)\n",
        "            precision_epoch, f1_epoch, recall_epoch = 0, 0, 0\n",
        "\n",
        "            optimal_thr = 0\n",
        "            for i in range(len(ground_truth_masks)):\n",
        "                f1_output, thr = optimal_f1(final_masks[i], ground_truth_masks[i])\n",
        "                f1_epoch += f1_output\n",
        "                optimal_thr += thr\n",
        "\n",
        "            wandb.log({\n",
        "                'w1': w1,\n",
        "                'w2': w2,\n",
        "                'w3': w3,\n",
        "                'best_f1': f1_epoch/len(validationloader),\n",
        "                'best_thr': optimal_thr/len(validationloader)\n",
        "            })\n",
        "\n",
        "            print(f1_epoch/len(validationloader))\n",
        "            print(optimal_thr/len(validationloader))\n",
        "\n",
        "\n",
        "\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JgoL1T4EoXbQ"
      },
      "outputs": [],
      "source": [
        "#gc_collect()\n",
        "#p_images, m_images, p_masks, m_masks = [], [], [], []\n",
        "#fit_all_images(polygon_numbers)\n",
        "#get_ensembling_scores()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1kci3-rcDHRK"
      },
      "outputs": [],
      "source": [
        "# todas son listas\n",
        "def hill_climbing(predicted_masks, ground_truth_masks, run_names):\n",
        "  '''Performs hill climbing ensembling technique. '''\n",
        "\n",
        "  i = 0\n",
        "  STOP = False\n",
        "  current_best_ensemble = (0.78 * (0.63 * (0.79 * (0.74 * (0.5 * predicted_masks[0] + 0.5 * predicted_masks[4]) + 0.26 * predicted_masks[3]) + 0.21 * predicted_masks[2]) + 0.37 * predicted_masks[1]) \\\n",
        "                          + 0.22 * predicted_masks[7])  # El mejor ensamblado de las máscaras\n",
        "  del predicted_masks[0], run_names[0], run_names[4], predicted_masks[4], run_names[3], predicted_masks[3], run_names[2], predicted_masks[2], run_names[1], predicted_masks[1], \\\n",
        "      predicted_masks[7], run_names[7]\n",
        "  history = [optimal_f1(current_best_ensemble, ground_truth_masks)[0]]\n",
        "  print('Initial best F1 Score: ', history[0])\n",
        "\n",
        "  while not STOP:\n",
        "    i+=1\n",
        "    # Calculamos el F1 Score del ensamblado de máscaras actual\n",
        "    potential_new_best_f1_score, potential_new_best_thr = optimal_f1(current_best_ensemble, torch.tensor(ground_truth_masks))\n",
        "    k_best, wgt_best = None, None\n",
        "    for k in range(len(predicted_masks)):\n",
        "        print('=== {} ==='.format(k))\n",
        "        for wgt in np.arange(-0.51,0.51,0.01):\n",
        "            # Añadimos las máscaras de un modelo al ensamblado\n",
        "            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * predicted_masks[k]\n",
        "            # Calculamos el nuevo F1 Score\n",
        "            f1_score, thr = optimal_f1(potential_ensemble, torch.tensor(ground_truth_masks))\n",
        "            # En caso de ser mejor, actualizamos valores\n",
        "            if f1_score > potential_new_best_f1_score:\n",
        "                potential_new_best_f1_score = f1_score\n",
        "                k_best, wgt_best = k, wgt\n",
        "                best_thr = thr\n",
        "\n",
        "    if k_best is not None:\n",
        "          # Si hemos conseguido aumentar el F1 Score, actualizamos el ensamblado de máscaras\n",
        "          current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * predicted_masks[k_best]\n",
        "          # Si ya no quedan más máscaras entonces paramos\n",
        "          if len(predicted_masks)==0:\n",
        "              STOP = True\n",
        "          # !!!!! Actualizar el print.\n",
        "          print(f'Iteration: {i}, Model added: {run_names[k_best]}, Best weight: {wgt_best:.2f}, Best Thr: {potential_new_best_thr}, Best F1: {potential_new_best_f1_score:.5f}')\n",
        "          history.append(potential_new_best_f1_score)\n",
        "          del predicted_masks[k_best], run_names[k_best]\n",
        "    # Si ningún ensamblado nuevo ha conseguido boostear el F1 Score => paramos\n",
        "    else:\n",
        "        STOP = True\n",
        "\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yvDukkrpnX4M",
        "outputId": "12c356bd-2243-435d-f6e6-d5e1f1771565"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230616_073650-gooe3smw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/javigallego4/Bachelor%20Thesis/runs/gooe3smw' target=\"_blank\">young-resonance-40</a></strong> to <a href='https://wandb.ai/javigallego4/Bachelor%20Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/javigallego4/Bachelor%20Thesis' target=\"_blank\">https://wandb.ai/javigallego4/Bachelor%20Thesis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/javigallego4/Bachelor%20Thesis/runs/gooe3smw' target=\"_blank\">https://wandb.ai/javigallego4/Bachelor%20Thesis/runs/gooe3smw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact morning-sweep-28:v0, 86.93MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.6\n",
            "100%|██████████| 183/183 [01:07<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:42<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:42<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact cool-sweep-26:v0, 79.49MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.2\n",
            "100%|██████████| 183/183 [01:42<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:43<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact zesty-sweep-9:v0, 83.99MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.7\n",
            "100%|██████████| 183/183 [00:41<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact driven-sweep-20:v0, 72.89MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.4\n",
            "100%|██████████| 183/183 [01:02<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [01:17<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:43<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:44<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact dark-sweep-66:v0, 50.79MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.1\n",
            "100%|██████████| 183/183 [00:44<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:43<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sleek-sweep-24:v0, 58.88MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.2\n",
            "100%|██████████| 183/183 [00:47<00:00,  3.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [01:03<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:41<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:40<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:44<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:39<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:42<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact firm-sweep-129:v0, 51.09MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.8\n",
            "100%|██████████| 183/183 [00:44<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:41<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:42<00:00,  4.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:54<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 183/183 [00:42<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted masks appended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:02<00:00, 73.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial best F1 Score:  tensor(0.9311)\n",
            "=== 0 ===\n",
            "=== 1 ===\n",
            "=== 2 ===\n",
            "=== 3 ===\n",
            "=== 4 ===\n",
            "=== 5 ===\n",
            "=== 6 ===\n",
            "=== 7 ===\n",
            "=== 8 ===\n",
            "=== 9 ===\n",
            "=== 10 ===\n",
            "=== 11 ===\n",
            "=== 12 ===\n",
            "=== 13 ===\n",
            "=== 14 ===\n",
            "=== 15 ===\n",
            "=== 16 ===\n",
            "=== 17 ===\n",
            "Iteration: 1, Model added: scarlet-sweep-159, Best weight: -0.18, Best Thr: -0.0699996173381805, Best F1: 0.93274\n",
            "=== 0 ===\n",
            "=== 1 ===\n",
            "=== 2 ===\n",
            "=== 3 ===\n",
            "=== 4 ===\n",
            "=== 5 ===\n",
            "=== 6 ===\n",
            "=== 7 ===\n",
            "=== 8 ===\n",
            "=== 9 ===\n",
            "=== 10 ===\n",
            "=== 11 ===\n",
            "=== 12 ===\n",
            "=== 13 ===\n",
            "=== 14 ===\n",
            "=== 15 ===\n",
            "=== 16 ===\n",
            "Iteration: 2, Model added: winter-sweep-24, Best weight: -0.16, Best Thr: -0.09999963343143459, Best F1: 0.93418\n",
            "=== 0 ===\n",
            "=== 1 ===\n",
            "=== 2 ===\n",
            "=== 3 ===\n",
            "=== 4 ===\n",
            "=== 5 ===\n",
            "=== 6 ===\n",
            "=== 7 ===\n",
            "=== 8 ===\n",
            "=== 9 ===\n",
            "=== 10 ===\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Init W&B\n",
        "run = wandb.init(entity=\"javigallego4\", project=\"Bachelor Thesis\", group = 'Baseline - Ensembling')\n",
        "\n",
        "# Preprocessing Pipeline (No initial scaling)\n",
        "p_images, m_images, p_masks, m_masks = [], [], [], []\n",
        "load_images(polygon_numbers)\n",
        "m_images, p_masks = preprocessing_pipeline('HLS', cv2.BORDER_CONSTANT, False)\n",
        "\n",
        "# Datasets and train-test-split\n",
        "multispectral_dataset = MultispectralDataset(m_images, p_masks)\n",
        "n = len(multispectral_dataset)\n",
        "validation_data = torch.utils.data.Subset(multispectral_dataset, np.concatenate([X_val, X_train]))\n",
        "gc_collect()\n",
        "validationloader = torch.utils.data.DataLoader(validation_data, batch_size = 16, num_workers = 2, pin_memory=True, shuffle=False)\n",
        "print('Dataset created')\n",
        "\n",
        "# Load artifact\n",
        "predicted_masks = []\n",
        "best_runs = ['morning-sweep-28','scarlet-sweep-159','effortless-sweep-175','cool-sweep-26','amber-sweep-21','zesty-sweep-9','driven-sweep-20','winter-sweep-24','bright-sweep-31','treasured-sweep-18','dark-sweep-66','golden-sweep-20','sleek-sweep-24',\\\n",
        "             'drawn-sweep-64', 'revived-sweep-9', 'atomic-sweep-78', 'electric-sweep-72', 'logical-sweep-10','dulcet-sweep-152','firm-sweep-129', 'honest-sweep-155', 'glamorous-sweep-124', 'kind-sweep-113', 'golden-sweep-174']\n",
        "n_filters_1 = [128, 96, 96, 512, 64, 64, 512, 512, 64, 128, 64, 128, 256, 512, 32, 16, 256, 32, 128, 96, 96, 32, 512, 32]\n",
        "n_filters_2 = [512, 64, 96, 512, 512, 512, 32, 256, 256, 256, 128, 512, 128, 128, 16, 64, 64, 16, 128, 256, 64, 256, 128, 96]\n",
        "n_filters_3 = [512, 512, 512, 64, 16, 512, 512, 64, 96, 128, 512, 64, 512, 128, 32, 512, 256, 32, 512, 512, 512, 512, 128, 512]\n",
        "n_filters_4 = [16, 256, 96, 64, 16, 32, 256, 16, 64, 96, 256, 128, 256, 256, 512, 96, 96, 256, 96, 16, 64, 16, 16, 96]\n",
        "\n",
        "#best_thr = [-0.1547, -0.1433, -0.1027]\n",
        "for i, run_name in enumerate(best_runs):\n",
        "    # Load artifact\n",
        "    artifact = run.use_artifact('javigallego4/Bachelor Thesis/{}:v0'.format(run_name), type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    # Load the model\n",
        "    model = multispectralUnet(n_filters_1[i], n_filters_2[i], n_filters_3[i], n_filters_4[i]).to(device)\n",
        "    model.load_state_dict(torch.load('/content/artifacts/{}:v0/{}.pt'.format(run_name, run_name)))\n",
        "\n",
        "    # Evaluate\n",
        "    predicted_masks.append(get_validation_masks(validationloader, model))\n",
        "    print('Predicted masks appended')\n",
        "\n",
        "ground_truth_masks = []\n",
        "for imgs, masks in tqdm(validationloader):\n",
        "    ground_truth_masks.append(masks.squeeze().to(device))\n",
        "\n",
        "ground_truth_masks = torch.cat(ground_truth_masks, dim = 0)[:,29:,:]\n",
        "hill_climbing(predicted_masks, ground_truth_masks, best_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhynS64OfGBI"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "# Init W&B\n",
        "run = wandb.init(entity=\"javigallego4\", project=\"Bachelor Thesis\", group = 'Baseline - Ensembling')\n",
        "\n",
        "# Preprocessing Pipeline (No initial scaling)\n",
        "p_images, m_images, p_masks, m_masks = [], [], [], []\n",
        "load_images(polygon_numbers)\n",
        "m_images, p_masks = preprocessing_pipeline('IHS3', cv2.BORDER_CONSTANT, False)\n",
        "\n",
        "# Datasets and train-test-split\n",
        "multispectral_dataset = MultispectralDataset(m_images, p_masks)\n",
        "n = len(multispectral_dataset)\n",
        "validation_data = torch.utils.data.Subset(multispectral_dataset, X_val)\n",
        "gc_collect()\n",
        "validationloader = torch.utils.data.DataLoader(validation_data, batch_size = 16, num_workers = 2, pin_memory=True, shuffle=False)\n",
        "print('Dataset created')\n",
        "\n",
        "# Load artifact\n",
        "predicted_masks = []\n",
        "best_runs = ['vague-sweep-19', 'apricot-sweep-60','trim-sweep-18','curious-sweep-71','royal-sweep-53','decent-sweep-68', 'super-sweep-62','elated-sweep-58']\n",
        "n_filters_1 = [96, 64, 96, 128, 32, 32, 128, 32]\n",
        "n_filters_2 = [512, 96, 32, 128, 128, 32, 16, 32]\n",
        "n_filters_3 = [64, 512, 512, 512, 256, 96, 64, 64]\n",
        "n_filters_4 = [32, 32, 96, 256, 96, 32, 512, 512]\n",
        "\n",
        "#best_thr = [-0.1547, -0.1433, -0.1027]\n",
        "for i, run_name in enumerate(best_runs):\n",
        "    # Load artifact\n",
        "    artifact = run.use_artifact('javigallego4/Bachelor Thesis/{}:v0'.format(run_name), type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    # Load the model\n",
        "    model = multispectralUnet(n_filters_1[i], n_filters_2[i], n_filters_3[i], n_filters_4[i]).to(device)\n",
        "    model.load_state_dict(torch.load('/content/artifacts/{}:v0/{}.pt'.format(run_name, run_name)))\n",
        "\n",
        "    # Evaluate\n",
        "    predicted_masks.append(get_validation_masks(validationloader, model))\n",
        "    print('Predicted masks appended')\n",
        "\n",
        "ground_truth_masks = []\n",
        "for imgs, masks in tqdm(validationloader):\n",
        "    ground_truth_masks.append(masks.squeeze().to(device))\n",
        "\n",
        "ground_truth_masks = torch.cat(ground_truth_masks, dim = 0)[:,29:,:]\n",
        "hill_climbing(predicted_masks, ground_truth_masks, best_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RKhwaKQEzXc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOGLHfA7QbFlNIScDyL5/I2",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}