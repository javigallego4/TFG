# -*- coding: utf-8 -*-
"""Segmentation Vegetation Idx + MaskRCNN (optimizado).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Otbnbak4bfdlDpbwUT12vOYBSjcTau0
"""

DEBUG = False
LOG_IMAGES = False
NEW_SWEEP = False

from multiprocessing import freeze_support
from IPython.display import clear_output, display_html
import gc; gc.enable()
import warnings
import os
from pathlib import Path
from tqdm import tqdm

# Basic libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import scipy as sc
from scipy import stats
import random

# Preprocessing libraries
from sklearn.preprocessing import *
import cv2
import albumentations as A

# Library for .tiff files
#!pip install tifffile
import tifffile as tiff

# Timm Library
#!pip install timm
import timm

# PyTorch 
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, random_split
import torchvision
from torchvision import transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau 
from PIL import Image

# MaskRCNN class imports
from typing import Any, Callable, Optional
from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor
from torch import nn
from torchvision.ops import MultiScaleRoIAlign
from torchvision.ops import misc as misc_nn_ops
from torchvision.transforms._presets import ObjectDetection

from torchvision.models.detection.mask_rcnn import MaskRCNN, MaskRCNNPredictor
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.models import *
from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor

# Deep Lab V3 Backbones
from torchvision.models.segmentation.deeplabv3 import *
from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101, deeplabv3_mobilenet_v3_large, DeepLabV3_ResNet101_Weights

# Metric (mAP)
#pip install torchmetrics
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# Weights and biases 
#!pip install wandb
import wandb

# Memory usage
import gc
def gc_collect():
    gc.collect()
    torch.cuda.empty_cache()

warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

clear_output()
print('Number of CPUs: ', os.cpu_count())

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

#print('MPS: ', torch.backends.mps.is_available())
print('CUDA: ', torch.cuda.is_available())
print('Device:', device)

train_validation_test_split = pd.read_csv('train_validation_test_split.csv', index_col = False)

"""Nos creamos los índices para los conjuntos de entrenamiento, validación y test."""

X_train = train_validation_test_split[train_validation_test_split['set'] == 'train'].image.values
X_test = train_validation_test_split[train_validation_test_split['set'] == 'test'].image.values
X_val = train_validation_test_split[train_validation_test_split['set'] == 'val'].image.values

"""# Metric: mean Average Precision (mAP)"""

if DEBUG: 
  # mAP BBOX
  preds = [
    dict(
      boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),
      scores=torch.tensor([0.536]),
      labels=torch.tensor([0]),
    )
  ]
  target = [
    dict(
      boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),
      labels=torch.tensor([0]),
    )
  ]
  metric = MeanAveragePrecision()
  metric.update(preds, target)
  print(metric.compute())

"""# Preprocessing Pipeline"""

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

# Obtenemos todos los nºs de polígonos que nos dan en la siguiente ruta. 

BASE_PATH = "WV3/"
polygon_numbers = os.listdir(BASE_PATH)
polygon_numbers = pd.Series(polygon_numbers).str.split('_', n = 2, expand = True)[1]
polygon_numbers = list(polygon_numbers)
polygon_numbers = sorted(polygon_numbers)
polygon_numbers.remove('Store')

# Guardamos en arrays cada una de las imágenes y máscaras.

def load_images(polygon_number):
  # Panchromatic Images
  p_images.append(tiff.imread(BASE_PATH + "polygon_{}/panchromatic.tif".format(polygon_number)))
  p_masks.append(tiff.imread(BASE_PATH + "polygon_{}/mask_panchromatic.tif".format(polygon_number)))
  
  # Multispectral Images
  # Hacemos un permute para poner las imágenes en el formato PyTorch
  m_images.append(tiff.imread(BASE_PATH + "polygon_{}/multispectral.tif".format(polygon_number)))
  m_masks.append(tiff.imread(BASE_PATH + "polygon_{}/mask_multispectral.tif".format(polygon_number)))

  return p_images, m_images, p_masks, m_masks

def fit_all_images(polygon_numbers):
  with ThreadPoolExecutor(1) as p:
      for i in tqdm(p.map(load_images, polygon_numbers), total=len(polygon_numbers)):
          pass

p_images, m_images, p_masks, m_masks = [], [], [], []
fit_all_images(polygon_numbers)

if DEBUG: 
  print('\nType: ', type(m_images[0]))
  print('Image Shape', m_images[0].shape)
  print('Mask Shape', m_masks[0].shape)

  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
  axes[0].imshow(m_images[1][:,:,:3])
  axes[1].imshow(m_masks[1])

  resized_image = cv2.resize(m_images[1], (215, 256), interpolation = cv2.INTER_NEAREST)
  resized_mask = cv2.resize(m_masks[1], (215, 256), interpolation = cv2.INTER_NEAREST)

  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
  axes[0].imshow(resized_image[:,:,:3])
  axes[1].imshow(resized_mask)

  resized_image = cv2.resize(resized_image, (21, 25), interpolation = cv2.INTER_NEAREST)
  resized_mask = cv2.resize(resized_mask, (21, 25), interpolation = cv2.INTER_NEAREST)

  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))
  axes[0].imshow(resized_image[:,:,:3])
  axes[1].imshow(resized_mask)

  # Transpose the image and mask
  image = m_images[1].transpose(1,0,2)
  mask = m_masks[1].transpose(1,0)

  # Flip the image and mask vertically
  image = cv2.flip(image, 1)
  mask = cv2.flip(mask, 1)

"""## Pansharpening"""

def inverse_pca_image(sharpened_img, pca):
  """
    Performs inverse PCA transformation to the given image.

    Parameters:
    - sharpened_img: image to be transformed
    - pca: PCA object with loadings from previous fit_transform()
  """

  X = sharpened_img.permute(1,2,0).numpy()

  b0 = X[:,:,0]
  b1 = X[:,:,1]
  b2 = X[:,:,2]
  b3 = X[:,:,3]
  b4 = X[:,:,4]
  b5 = X[:,:,5]
  b6 = X[:,:,6]
  b7 = X[:,:,7]

  pca_df = pd.DataFrame({'B0': b0.flatten(), 'B1': b1.flatten(), 'B2': b2.flatten(), 'B3':b3.flatten(), 
              'B4': b4.flatten(), 'B5': b5.flatten(), 'B6': b6.flatten(), 'B7':b7.flatten()})

  img = pca.inverse_transform(pca_df)

  for i in range(8): 
    sharpened_img[i, :, :] = torch.from_numpy(img[:,i].reshape((sharpened_img.shape[1], sharpened_img.shape[2])))

  return sharpened_img

def histogram_match(pan, band):
    """
    Performs histogram matching between the panchromatic image and the multispectral band given.

    Parameters:
    - pan: torch tensor of shape (height, width)
    - band: torch tensor of shape (height, width)

    Returns:
    - matched_panchromatic: histogram matched PAN imagery
    """

    # Fórmula UGR: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwicxYCXrYv9AhXRhqQKHYoUDFQQFnoECAkQAQ&url=https%3A%2F%2Fccia.ugr.es%2Fvip%2Ffiles%2Fbooks%2Fpaper_amro_mateos.pdf&usg=AOvVaw3wn01QiErGCJLtNZUg-oKe
    matched_panchromatic = (pan - pan.mean()) * (band.std() / pan.std()) + band.mean()

    return matched_panchromatic

def pansharpen_image(multispectral_image, panchromatic_image, method):
    """
    Pansharpens the given MS image based on different techniques.

    Parameters:
    - multispectral_image: torch tensor of shape (channels, height, width)
    - panchromatic_image: torch tensor of shape (height, width)
    - method: type of pansharpening technique

    Returns:
    - sharpened_img: torch tensor with same shape as input multispectral image
    """
    
    if method == 'PCA': 
      # Forward Transform: perform PCA and replace 1st component with panchromatic band
      sharpened_img, pca = pca_image(multispectral_image, 8, True)
      sharpened_img = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(sharpened_img)

      # Histogram Matching: the PAN Imagery is matched with the PC1 band
      matched_panchromatic = histogram_match(panchromatic_image, sharpened_img[0, :, :])

      # Component Substitution
      sharpened_img[0, :, :] = matched_panchromatic

      # Reverse Transform: undo PCA
      sharpened_img = inverse_pca_image(sharpened_img, pca)
      
    if method == 'Simple Mean':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])
      for i in range(multispectral_image.shape[0]):
        # Histogram Matching for each band
        matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])        
        sharpened_img[i, :, :] = 0.5 * (multispectral_image[i, :, :] + matched_panchromatic)

    if method == 'Brovey': 
        multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
        sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])

        M = 0
        for i in range(multispectral_image.shape[0]):
          M += multispectral_image[i, :, :]
        M /= multispectral_image.shape[0]

        for i in range(multispectral_image.shape[0]):
          # Histogram Matching for each band
          matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])
          sharpened_img[i, :, :] = (matched_panchromatic / M) * multispectral_image[i, :, :]

    if method == 'HSV': 
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 0.577 * (red+green+blue)
      v1 = -0.408 * (red+green) + 0.816 * blue
      v2 = -0.707 * (red+green) + 1.703 * blue

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 0.577 * matched_panchromatic - 0.408 * v1 - 0.707 * v2
      new_green = 0.577 * matched_panchromatic - 0.408 * v1 - 0.816 * v2
      new_blue = 0.577 * matched_panchromatic - 0.816 * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS1':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/np.sqrt(3) * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * (-red+green)

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      v1 = S * torch.cos(H)
      v2 = S * torch.sin(H)

      new_red = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 - 1/np.sqrt(2) * v2
      new_green = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_blue = 1/np.sqrt(3) * matched_panchromatic + 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS2':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(6) * red - 2 /np.sqrt(6) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      v1 = S * torch.cos(2*np.pi*H)
      v2 = S * torch.sin(2*np.pi*H)

      new_red = 1 * matched_panchromatic -0.204124 * v1 - 0.612372 * v2
      new_green = 1 * matched_panchromatic -0.204124 * v1 + 0.612372 * v2
      new_blue = 1 * matched_panchromatic + 0.408248 * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS3':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(6) * red - 1/np.sqrt(6) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic -1/np.sqrt(6) * v1 +3/np.sqrt(6) * v2
      new_green = 1 * matched_panchromatic -1/np.sqrt(6) * v1 -3/np.sqrt(6) * v2
      new_blue = 1 * matched_panchromatic + 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS4':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v1/v2)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 -1/np.sqrt(2) * v2
      new_blue = 1/3 * matched_panchromatic - 1/np.sqrt(2) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS5':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS6':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -2/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic -1/np.sqrt(2) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic -1/np.sqrt(2) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic + np.sqrt(2) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'HLS':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v1/v2)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    return sharpened_img

def pansharpening(method):
  """
  Performs pansharpening to every image in the dataset.
  """

  for i in range(len(p_images)):
    img = pansharpen_image(torch.from_numpy(m_images[i]).permute(2,0,1), torch.from_numpy(p_images[i]), method)
    m_images[i] = img.permute(1,2,0).numpy()

"""## Scaling"""

def scale_panchromatic_image(image, transformer = MinMaxScaler()):
  '''Returns input panchromatic image with its values being scaled to the [0,1] interval. '''

  img = transformer.fit_transform(image, )
  return img

def scale_multispectral_image(image, bands = 8, transformer = MinMaxScaler()):
  '''Returns input multispectral image with its values being scaled to the [0,1] interval. '''

  b0 = image[:,:,0]
  b1 = image[:,:,1]
  b2 = image[:,:,2]
  if bands == 8: 
    b3 = image[:,:,3]
    b4 = image[:,:,4]
    b5 = image[:,:,5]
    b6 = image[:,:,6]
    b7 = image[:,:,7]

  # As before, we perform some scaling first
  sc = transformer
  b0 = sc.fit_transform(b0)
  b1 = sc.fit_transform(b1)
  b2 = sc.fit_transform(b2)
  if bands == 8: 
    b3 = sc.fit_transform(b3)
    b4 = sc.fit_transform(b4)
    b5 = sc.fit_transform(b5)
    b6 = sc.fit_transform(b6)
    b7 = sc.fit_transform(b7)

  if bands == 8: img = np.dstack([b0, b1, b2, b3, b4, b5, b6, b7])
  else: img = np.dstack([b0, b1, b2])
  return img

def scaling():
  '''Pipeline function for value scaling. '''

  for i in range(len(p_images)):
    p_images[i] = scale_panchromatic_image(p_images[i])
    m_images[i] = scale_multispectral_image(m_images[i])

"""## Data Augmentation"""

def HorizontalFlip():
  '''Performs horizontal flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], 1))
    p_masks.append(cv2.flip(p_masks[i], 1))

def VerticalFlip():
  '''Performs vertical flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], 0))
    p_masks.append(cv2.flip(p_masks[i], 0))

def VHFlip(): 
  '''Performs both horizontal and vertical flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], -1))
    p_masks.append(cv2.flip(p_masks[i], -1))

def Rotation90():
  '''Performs a 90 degrees rotation on the images'''

  for i in range(len(polygon_numbers)):

    # Transpose the image
    image = image.transpose(1,0,2)
    # Flip the image vertically
    image = cv2.flip(image, 1)
    m_images.append(cv2.flip(m_images[i], -1))
    p_masks.append(cv2.flip(p_masks[i], -1))
  
# source: https://www.kaggle.com/safavieh/image-augmentation-using-skimage
import random
import pylab as pl 

def random_crop(img, mask, crop_height, crop_width):
    
    height, width = img.shape[0], img.shape[1]

    # Calculate aspect ratio
    aspect_ratio = float(width / height)

    # Calculate the maximum width and height that can be cropped while maintaining the aspect ratio
    max_crop_width = int(aspect_ratio * crop_height)
    max_crop_height = int(crop_width / aspect_ratio)

    # Choose a random starting point for the crop
    start = random.randint(0,10)
    x = random.randint(start, width - max_crop_width)
    y = random.randint(start, height - max_crop_height)

    # Crop the image and mask
    cropped_img = img[y:y+max_crop_height, x:x+max_crop_width]
    cropped_mask = mask[y:y+max_crop_height, x:x+max_crop_width]

    # Resize the cropped image and mask to the desired size
    # Interpolation. Possible values: cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_AREA, cv2.INTER_CUBIC
    resized_image = cv2.resize(cropped_img, (width, height), interpolation=cv2.INTER_LINEAR)
    resized_mask = cv2.resize(cropped_mask, (width, height), interpolation=cv2.INTER_LINEAR)

    return resized_image, resized_mask

def RandomCropping(): 
    ''' Applying 10 random croppings to all the images.'''

    for i in range(len(polygon_numbers)):
      for j in range(10):
          width = random.randint(40, 60)
          aspect_ratio = m_images[i].shape[0] / m_images[i].shape[1]
          img, mask = random_crop(m_images[i], p_masks[i], int(width*aspect_ratio), width)
          m_images.append(img)
          p_masks.append(mask)

def data_augmentation():
  '''Performs data augmentation over images and masks. '''

  HorizontalFlip()
  VerticalFlip()
  VHFlip()
  RandomCropping()
  #GaussianBlur()

"""## Padding"""

def pad_images(imgs, msks, border):
  '''Pipeline function for images' padding. '''

  border_type = border
  images, masks = [], []
  for i in range(len(imgs)):
    images.append(cv2.copyMakeBorder(imgs[i], 99 - imgs[i].shape[0], 0, 80 - imgs[i].shape[1], 0, border_type))
    masks.append(cv2.copyMakeBorder(msks[i], 99 - msks[i].shape[0], 0, 80 - msks[i].shape[1], 0, border_type))

  return images, masks

"""## Preprocessing Pipeline Main Function"""

def preprocessing_pipeline(method, border_type = cv2.BORDER_CONSTANT, scale = True): 
  for i in range(len(m_images)):
    m_images[i] = m_images[i].astype(float)
    p_images[i] = p_images[i].astype(float)
  
  if scale: scaling()
  pansharpening(method)
  data_augmentation()
  imgs, masks = pad_images(m_images, p_masks, border_type)

  for i in range(len(imgs)):
    masks[i] = masks[i].astype(int)

  return imgs, masks

#m_images, p_masks = preprocessing_pipeline('Simple Mean')

"""# PyTorch Dataset"""

# Commented out IPython magic to ensure Python compatibility.
#from sklearn.preprocessing import *
from sklearn.decomposition import PCA

def apply_pca(X, transformer = False, components = -1):
    aux = X.copy()
    if transformer:
        X = pd.DataFrame(transformer.fit_transform(X))
        X.columns = aux.columns    
    # Create principal components
    if components == -1:
        pca = PCA()
    else:
        pca = PCA(n_components = components)
        
    X_pca = pca.fit_transform(X)
    # Convert to dataframe
    component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
    X_pca = pd.DataFrame(X_pca, columns=component_names)
    # Create loadings
    loadings = pd.DataFrame(
        pca.components_.T,  # transpose the matrix of loadings
        columns=component_names,  # so the columns are the principal components
        index=X.columns,  # and the rows are the original features
    )
    return pca, X_pca, loadings

def plot_variance(pca, width=8, dpi=100):
    # Create figure
    fig, axs = plt.subplots(1, 2, sharey = True)
    n = pca.n_components_
    grid = np.arange(1, n + 1)
    # Explained variance
    evr = pca.explained_variance_ratio_
    axs[0].bar(grid, evr)
    axs[0].set(
        xlabel="Component", title="% Explained Variance")    # ylim = (0.0, 1.0) o sino sharey en plt.subplots
    cv = np.cumsum(evr)
    axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")
    axs[1].set(
        xlabel="Component", title="% Cumulative Variance"
    )
    # Set up figure
    fig.set(figwidth=8, dpi=100)
    return axs

def pca_image(img, components = 3, return_pca = False): 
  ''' Performs a PCA and returns a 3-band image with the more significant bands. '''

  image = img.permute(1,2,0).numpy()

  b0 = image[:,:,0]
  b1 = image[:,:,1]
  b2 = image[:,:,2]
  b3 = image[:,:,3]
  b4 = image[:,:,4]
  b5 = image[:,:,5]
  b6 = image[:,:,6]
  b7 = image[:,:,7]

  pca_df = pd.DataFrame({'B0': b0.flatten(), 'B1': b1.flatten(), 'B2': b2.flatten(), 'B3':b3.flatten(), 
              'B4': b4.flatten(), 'B5': b5.flatten(), 'B6': b6.flatten(), 'B7':b7.flatten()})
  
  global_pca, X_pca, loadings = apply_pca(pca_df)

  #X_pca = global_pca.transform(pca_df)
  component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
  X_pca = pd.DataFrame(X_pca, columns=component_names)

  sc = MinMaxScaler()
  if components == 3: 
    img = X_pca.loc[:,:'PC3']
    img = pd.DataFrame(sc.fit_transform(img))
    img = img.values.reshape((image.shape[0], image.shape[1], 3))
  
  elif components == 8: 
    img = X_pca
    img = pd.DataFrame(sc.fit_transform(img))
    img = img.values.reshape((image.shape[0], image.shape[1], 8))

  if return_pca == True: return torch.from_numpy(img.astype(float)).permute(2,0,1), global_pca
  else: return torch.from_numpy(img.astype(float)).permute(2,0,1)

def vegetation_indexes(img):
  ''' Returns 3 selected vegetation indexes for the image given.

#       %0	Coastal	400	425	450	50	1.24	
#       %1	Blue	450	480	510	60	1.24	
#       %2	Green	510	545	580	70	1.24	
#       %3	Yellow	585	605	625	40	1.24	
#       %4	Red	630	660	690	60	1.24	
#       %5	Red Edge	705	725	745	40	1.24	
#       %6	Near-IR1	770	832.5	895	125	1.24	
#       %7	Near-IR2	860	950	1040	180	1.24

      * Normalized Difference Vegetation Index (NDVI) -> (NIR2-Red)/(NIR2 + Red)	UAV/WV-3
      * Green Normalized Difference Vegetation Index  -> (GNDVI)	(NIR2-Green)/(NIR2 + Green)	UAV/WV-3
      * Structure Insensitive Pigment Index (SIPI)	  ->  (NIR1-Blue)/(NIR1 + Red)	WV-3'''

  # Colour Bands
  nir1 = img[6,:,:]
  nir2 = img[7,:,:]
  red = img[4,:,:]
  green = img[2,:,:]
  blue = img[1,:,:]

  # Vegetation Indexes
  ndvi = (nir2 - red) / (nir2 + red)
  gndvi = (nir2 - green) / (nir2 + green)
  sipi = (nir1 - blue) / (nir1 + red)

  img = torch.stack([ndvi, gndvi, sipi], axis = 0)
  img = torch.clamp(img, min=-3, max=3)
  return img

def extract_image_targets(m, train = False, areas_to_drop = 16):
  ''' Given the mask of a multispectral image, it returns both the instance masks list
      and their respective bounding boxes coordinates. '''

  num_components, masks, stats, centroids = cv2.connectedComponentsWithStats(m.to(torch.uint8).numpy())

  # Find the unique labels in the mask
  labels, counts = torch.unique(torch.from_numpy(masks), return_counts=True)

  # Create an empty list to store the instance masks
  instance_masks = []

  if train: 
    # Iterate over the unique labels and create a separate mask for each one
    for label in labels[1:]:
        # create a mask with only the current label
        label_mask = torch.from_numpy(masks) == label
        # count the number of ones in the mask
        num_ones = torch.sum(label_mask)
        # check if the number of ones is greater than 8
        if num_ones > areas_to_drop:
            instance_masks.append(torch.where(label_mask, torch.ones_like(torch.from_numpy(masks)), torch.zeros_like(torch.from_numpy(masks))))
  else: 
    # Iterate over the unique labels and create a separate mask for each one
    for label in labels[1:]:
        instance_mask = torch.where(torch.from_numpy(masks) == label, torch.ones_like(torch.from_numpy(masks)), torch.zeros_like(torch.from_numpy(masks)))
        instance_masks.append(instance_mask)

  # Obtain bounding boxes coordinates
  boxes = torch.zeros([len(instance_masks),4], dtype=torch.float32)
  for i in range(len(instance_masks)):
      x,y,w,h = cv2.boundingRect(instance_masks[i].numpy().astype(np.uint8))
      boxes[i] = torch.tensor([x-1, y-1, x+w, y+h])

  return instance_masks, boxes

def show_masks_with_boxes(instance_masks, boxes): 
  ''' Shows a plot with each object mask, bounded by a rectangle. '''

  n_objects = len(instance_masks)
  plt.figure(figsize=(4*n_objects, 4))
  for i in range(n_objects):
    plt.subplot(1,n_objects, i+1)
    plt.imshow(instance_masks[i])

    # Draw the bounding rectangle
    x1, y1, x2, y2 = boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]
    plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, color='r'))
  plt.show()

class PanchromaticDataset(torch.utils.data.Dataset):
    def __init__(self, images, masks):
      super().__init__()
      self.images = images 
      self.masks = masks 

    def __getitem__(self, i):
      return torch.from_numpy(self.images[i].astype(float)), torch.from_numpy(self.masks[i].astype(float))
        
    def __len__(self):
      return len(self.images)

class MultispectralDataset(torch.utils.data.Dataset):
  def __init__(self, images, masks, approach):
    super().__init__()
    self.images = images 
    self.masks = masks 
    self.approach = approach
    #self.size = size

  def __getitem__(self, i):
    img, mask = torch.from_numpy(self.images[i].astype(float)).permute(2,0,1), torch.from_numpy(self.masks[i].astype(float))
    
    if self.approach == 'Vegetation Indexes': img = torch.nan_to_num(vegetation_indexes(torch.nan_to_num(img)))
    if self.approach == 'PCA': img = pca_image(torch.nan_to_num(img))

    return img, mask

  def __len__(self):
      return len(self.images)


"""# Model"""

"""### Model Implementation"""

import torchvision.models as models
if NEW_SWEEP == False: torch.hub.set_dir('/mnt/homeGPU/jgallego/models')

from torchvision.models.detection import maskrcnn_resnet50_fpn, maskrcnn_resnet50_fpn_v2

class maskrcnn_model(nn.Module):
    def __init__(self, b):
        super().__init__()

        if b in ['resnet50-fpn','resnet50-fpn-v2']: 

          if b == 'resnet50-fpn': self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights = 'DEFAULT') 
          else: self.model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights = 'DEFAULT') 
               
          # Replace the classifier with a new one, that has num_classes which is user-defined
          num_classes = 2  # In this case two classes: "jeniperus" and "background"
          in_features = self.model.roi_heads.box_predictor.cls_score.in_features # get number of input features for the classifier
          self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # replace the pre-trained head with a new one

          # Change the anchor generator
          self.model.roi_heads.box_predictor.anchor_generator = AnchorGenerator(
              sizes=((2, 4, 8, 16, 32, 64, 128, 256, 512),),
              aspect_ratios=((0.125, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0),)
          )

          # now get the number of input features for the mask classifier
          in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels
          hidden_layer = 256
          # and replace the mask predictor with a new one
          self.model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

        else: 

          backbone = timm.create_model(b, pretrained = True)
          out_channels = backbone.get_classifier().in_features
          backbone = nn.Sequential(*list(backbone.children())[:-2])
          backbone.out_channels = out_channels

          anchor_generator = AnchorGenerator(
              sizes=((2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048),),
              aspect_ratios=((1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64, 128),)
          )

          box_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],
                                                          output_size=4,
                                                          sampling_ratio=2)
          
          mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=8, sampling_ratio=2)
          
          # put the pieces together inside a MaskRCNN model
          self.model = MaskRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=box_roi_pooler, mask_roi_pool=mask_roi_pooler)

          # Change the anchor generator from the predictor
          self.model.roi_heads.box_predictor.anchor_generator = AnchorGenerator(
              sizes=(tuple([i for i in range(1,300)]),),
              aspect_ratios=(tuple([1/i for i in range(2,300)] + [i for i in range(1,300)]),)
          )
        
    def forward(self, inputs, targets):
        x = self.model(inputs, targets)
        return x
      
    def predict(self, x):
        x = self.model(x)
        return x

"""# Training Pipeline

### Helper Functions
"""

from torchmetrics import F1Score, Precision, Recall

def optimal_f1(predictions, targets):
    '''With this function we obtain the optimal threshold, given our model predictions. '''
    
    thres = np.linspace(torch.min(predictions).item(), torch.max(predictions).item(), 101)
    f1_score = F1Score(task="binary", num_classes=2).to(device)
    f1s = torch.Tensor([f1_score(predictions > thr, targets) for thr in thres])
    idx = torch.argmax(f1s)
    return f1s[idx], thres[idx]

class EarlyStopping():
    """
    Early stopping to stop the training when the loss does not improve after
    certain epochs.
    """
    def __init__(self, patience=5, min_delta=0):
        """
        :param patience: how many epochs to wait before stopping when loss is
               not improving
        :param min_delta: minimum difference between new loss and old loss for
               new loss to be considered as an improvement
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        
    def __call__(self, val_loss, val_mAP):
        if val_mAP <= 0.001: 
            print('Very low mAP\n\n')
            print('INFO: Early stopping')
            self.early_stop = True

        if self.best_loss == None:
            self.best_loss = val_loss
            
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            # reset counter if validation loss improves
            self.counter = 0
            
        elif self.best_loss - val_loss <= self.min_delta:
            self.counter += 1
            print(f"INFO: Early stopping counter {self.counter} of {self.patience}")
            if self.counter >= self.patience:
                print('INFO: Early stopping\n\n')
                self.early_stop = True

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True

def train_one_epoch(train_loader, model, optimizer, scaler, drop_little = False, drop_little_area = 16):
    # Track losses
    loss_epoch = 0
    mAP_epoch, mAP50_epoch, mAP75_epoch = 0, 0, 0
    precision_score = Precision(task="binary", num_classes=2).to(device)
    recall_score = Recall(task="binary", num_classes=2).to(device)
    precision_epoch, f1_epoch, recall_epoch = 0, 0, 0
    thr_list = []

    # Loop over minibatches
    for imgs, masks in tqdm(train_loader):
        model.train() # Train mode

        # Create MaskRCNN inputs and targets
        inputs = [imgs[i].to(device, dtype = torch.float32, non_blocking = True) for i in range(imgs.shape[0])]
        targets = []
        del_image = []

        for i in range(imgs.shape[0]):
          instance_masks, boxes = extract_image_targets(masks[i].squeeze(), drop_little, drop_little_area)

          if len(instance_masks) == 0: 
              del_image.append(i)
          else: 
              target = {}
              target["boxes"] = boxes.to(device, non_blocking = True)
              target["labels"] = torch.ones(len(instance_masks), dtype=torch.int64).to(device, non_blocking = True)   
              target["masks"] = torch.stack(instance_masks).to(torch.uint8).to(device, non_blocking = True)
              targets.append(target)

        train_inputs = [elem for i, elem in enumerate(inputs) if i not in del_image]
        if len(train_inputs) != 0:

          # Zero gradients
          optimizer.zero_grad(set_to_none=True)

          # Forward pass
          with torch.cuda.amp.autocast():
            outputs = model(train_inputs, targets)

          losses = sum(loss for loss in outputs.values())

          # Scales the loss, and calls backward() to create scaled gradients
          scaler.scale(losses).backward()

          # Unscales gradients and calls or skips optimizer.step()
          scaler.step(optimizer)

          # Updates the scale for next iteration
          scaler.update()
              
          # Track losses
          loss_epoch += losses.detach().item()

          # Track metric. Don't update weights
          with torch.no_grad():
            model.eval()
            pred = model.predict(inputs)

            inputs_pred = []
            masks_list = []

            for i in range(len(inputs)):
              masks_list.append(torch.sum(pred[i]['masks'], dim=0))
              p = dict(
                boxes=pred[i]['boxes'],
                scores=pred[i]['scores'],
                labels=pred[i]['labels'],
              )
              inputs_pred.append(p)

            masks_pred = torch.stack(masks_list, 0).squeeze() 
            for i, mask in enumerate(masks_pred): 
              f1_output, threshold = optimal_f1(mask, masks[i].squeeze().to(device)) 
              thr_list.append(threshold.item())
              f1_epoch += f1_output 
              recall_epoch += recall_score(mask > threshold, masks[i].squeeze().to(device)) 
              precision_epoch += precision_score(mask > threshold, masks[i].squeeze().to(device)) 
            
            targets_pred = []

            for i in range(imgs.shape[0]):
              instance_masks, boxes = extract_image_targets(masks[i].squeeze(), True)
              target = {}
              target["boxes"] = boxes.to(device)
              target["labels"] = torch.ones(len(instance_masks), dtype=torch.int64).to(device)   
              targets_pred.append(target)

            metric = MeanAveragePrecision(iou_type = 'bbox') 
            metric.update(inputs_pred, targets_pred)
            m = metric.compute()
            mAP_epoch += m['map']
            mAP50_epoch += m['map_50']
            mAP75_epoch += m['map_75']

            del inputs, targets, inputs_pred
            gc_collect()

    return loss_epoch/len(train_loader), mAP_epoch/len(train_loader), mAP50_epoch/len(train_loader), mAP75_epoch/len(train_loader), f1_epoch/len(train_loader), precision_epoch/len(train_loader), recall_epoch/len(train_loader), sum(thr_list)/len(thr_list), model

def validate_one_epoch(validation_loader, model, thr):
    #metric = MeanAveragePrecision(iou_type = 'bbox')
    loss_epoch = 0
    mAP_epoch, mAP50_epoch, mAP75_epoch = 0, 0, 0
    f1_score = F1Score(task="binary", num_classes=2).to(device)
    precision_score = Precision(task="binary", num_classes=2).to(device)
    recall_score = Recall(task="binary", num_classes=2).to(device)
    precision_epoch, f1_epoch, recall_epoch = 0, 0, 0
    
    # Don't update weights
    with torch.no_grad():

      # Loop over minibatches
      for imgs, masks in tqdm(validation_loader):
          model.train()

          # Create MaskRCNN inputs and targets
          inputs = [imgs[i].to(device, dtype = torch.float32) for i in range(imgs.shape[0])]
          targets = []

          for i in range(imgs.shape[0]):
            instance_masks, boxes = extract_image_targets(masks[i].squeeze())
            target = {}
            target["boxes"] = boxes.to(device)
            target["labels"] = torch.ones(len(instance_masks), dtype=torch.int64).to(device)   
            target["masks"] = torch.stack(instance_masks).to(torch.uint8).to(device)
            targets.append(target)

          # Make predictions and obtain losses
          with torch.cuda.amp.autocast():
            outputs = model(inputs, targets)

          losses = sum(loss for loss in outputs.values())

          # Track loss
          loss_epoch += losses.detach().item()

          # Track metric
          model.eval()
          pred = model.predict(inputs)

          inputs_pred = []
          masks_list = []

          for i in range(imgs.shape[0]):
            masks_list.append(torch.sum(pred[i]['masks'], dim=0))
            p = dict(
              boxes=pred[i]['boxes'],
              scores=pred[i]['scores'],
              labels=pred[i]['labels'],
              #masks=pred[i]['masks'].squeeze().to(torch.uint8) if pred[i]['masks'].shape[0] != 1 \
              #      else pred[i]['masks'].squeeze().unsqueeze(0).to(torch.uint8)
            )
            inputs_pred.append(p)

          masks_pred = torch.stack(masks_list, 0).squeeze() 
          f1_output = f1_score(masks_pred > thr, masks.squeeze().to(device)) 
          f1_epoch += f1_output 
          recall_epoch += recall_score(masks_pred > thr, masks.squeeze().to(device)) 
          precision_epoch += precision_score(masks_pred > thr, masks.squeeze().to(device)) 

          targets_pred = []
          for i in range(imgs.shape[0]):
            instance_masks, boxes = extract_image_targets(masks[i].squeeze())
            target = {}
            target["boxes"] = boxes.to(device)
            target["labels"] = torch.ones(len(instance_masks), dtype=torch.int64).to(device)   
            targets_pred.append(target)

          metric = MeanAveragePrecision(iou_type = 'bbox') 
          metric.update(inputs_pred, targets_pred)
          m = metric.compute()
          mAP_epoch += m['map']
          mAP50_epoch += m['map_50']
          mAP75_epoch += m['map_75']

          del inputs, targets, inputs_pred
          gc_collect()
            
    return loss_epoch/len(validation_loader), mAP_epoch/len(validation_loader), mAP50_epoch/len(validation_loader), mAP75_epoch/len(validation_loader), f1_epoch/len(validation_loader), precision_epoch/len(validation_loader), recall_epoch/len(validation_loader)
    #return loss_epoch/len(validation_loader), pfbeta_epoch/len(validation_loader), threshold


"""### Main Function"""

print('WANDB Logging')
if NEW_SWEEP == False: os.environ['WANDB_DIR'] = '/mnt/homeGPU/jgallego/'
if NEW_SWEEP == False: os.environ['WANDB_CONFIG_DIR'] = '/mnt/homeGPU/jgallego/'
if NEW_SWEEP == False: os.environ['WANDB_CACHE_DIR'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'
if NEW_SWEEP == False: os.environ['WANDB_ARTIFACT_STAGING'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'
if NEW_SWEEP == False: os.environ['WANDB_DATA_DIR'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'

os.environ["WANDB_API_KEY"] = '5bf911e7e682da23240c68fb146a222bf0475f7c'
wandb.login() # 5bf911e7e682da23240c68fb146a222bf0475f7c
print('Logging DONE\n')

import time

def add_weight_decay(model, weight_decay=1e-5, skip_list=()):
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):
            # print(name, 'no decay')
            no_decay.append(param)
        else:
            # print(name, 'decay')
            decay.append(param)
    return [
        {'params': no_decay, 'weight_decay': 0.},
        {'params': decay, 'weight_decay': weight_decay}]

def train_model(verbose=True):
    torch.manual_seed(42)
    # Init W&B 
    run = wandb.init(dir='/mnt/homeGPU/jgallego/', group = 'VI Tuning')
    if NEW_SWEEP == False: os.system('chmod -R 777 /mnt/homeGPU/jgallego/')

    # Preprocessing Pipeline (No initial scaling)
    m_images2, p_masks2 = preprocessing_pipeline(wandb.config.pansharpening, cv2.BORDER_REFLECT_101, wandb.config.scaling)

    # Datasets and train-test-split
    multispectral_dataset = MultispectralDataset(m_images2, p_masks2, wandb.config.dimensionality_reduction)
    n = len(multispectral_dataset)
    training_data = torch.utils.data.Subset(multispectral_dataset, X_train)
    validation_data = torch.utils.data.Subset(multispectral_dataset, X_val)
    test_data = torch.utils.data.Subset(multispectral_dataset, X_test)
    gc_collect()

    print('Dataset created')

    # Model
    model = maskrcnn_model(wandb.config.backbone).to(device)
    print('Model created')
    
    # Freezing initial layers
    NUM_FROZEN_LAYERS = int(len(list(model.named_parameters())) * wandb.config.freezing) # how many layers you want to freeze
    for name, param in list(model.named_parameters())[0:NUM_FROZEN_LAYERS]:
        param.requires_grad = False

    # Construct an optimizer and scheduler
    #params = [p for p in model.parameters() if p.requires_grad]
    # Weight Decay = L2 regularization
    params = add_weight_decay(model, weight_decay=wandb.config.weight_decay, skip_list=['bias'])

    if wandb.config.optimizer == 'AdamW': 
      optimizer = optim.AdamW(params, lr = wandb.config.lr, weight_decay = wandb.config.weight_decay, amsgrad = wandb.config.amsgrad) 
    elif wandb.config.optimizer == 'Adam': 
      optimizer = optim.Adam(params, lr = wandb.config.lr, weight_decay = wandb.config.weight_decay, amsgrad = wandb.config.amsgrad) 
    
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=wandb.config.reducing_factor, patience=3, threshold = wandb.config.threshold, verbose = True)

    # Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.
    scaler = torch.cuda.amp.GradScaler()
    
    batch = wandb.config.batch_size 
    trainloader = torch.utils.data.DataLoader(training_data, batch_size = batch, num_workers = 64, shuffle = True, pin_memory=True)
    validationloader = torch.utils.data.DataLoader(validation_data, batch_size = batch, num_workers = 64, pin_memory=True)
    testloader = torch.utils.data.DataLoader(test_data, batch_size = batch, num_workers = 64, pin_memory=True)

    best_mAP = 0
    early_stopping = EarlyStopping(10, wandb.config.threshold)

    print('Preprocessing done')

    # Loop over epochs
    for epoch in range(wandb.config.epochs):
            
          # Train
          train_loss, train_mAP, train_mAP50, train_mAP75, train_f1, train_precision, train_recall, thr, model = train_one_epoch(trainloader, model, optimizer, scaler, wandb.config.drop_little_objets, wandb.config.drop_objects_area)
          
          # Evaluate
          val_loss, val_mAP, val_mAP50, val_mAP75, val_f1, val_precision, val_recall = validate_one_epoch(validationloader, model, thr)

          # Apply scheduler
          scheduler.step(val_loss)
          
          # Log metrics
          wandb.log({
              'epoch': epoch,
              'train_loss': train_loss,
              'val_loss': val_loss,
              'train_mAP': train_mAP,
              'train_mAP50': train_mAP50, 
              'train_mAP75': train_mAP75, 
              'train_f1': train_f1, 
              'train_precision': train_precision, 
              'train_recall': train_recall,
              'val_mAP': val_mAP,
              'val_mAP50': val_mAP50,
              'val_mAP75': val_mAP75, 
              'val_f1': val_f1, 
              'val_precision': val_precision, 
              'val_recall': val_recall, 
              'threshold': thr
          })

          # Print loss
          if verbose:
              if (epoch+1)%1==0:
                  print(f'\nEpoch {epoch+1}/{wandb.config.epochs}')
                  print(f'loss {train_loss:.5f}, val_loss {val_loss:.5f}')
                  print(f'mAP {train_mAP:.5f}, mAP50 {train_mAP50:.5f}, mAP75 {train_mAP75:.5f}, val_mAP {val_mAP:.10f}, val_mAP50 {val_mAP50:.5f}, val_mAP75 {val_mAP75:.5f}')
                  print(f'f1 {train_f1:.5f}, precision {train_precision:.5f}, recall {train_recall:.5f}, val_f1 {val_f1:.5f}, val_precision {val_precision:.5f}, val_recall {val_recall:.5f}')  
                  print(f'Threshold {thr}')
  
          # Early Stopping
          early_stopping(val_loss, val_mAP)
          if early_stopping.early_stop:
              break
          else:
              if val_mAP > best_mAP: 
                  best_mAP = val_mAP
                  best_thr = thr
                  best_model_state_dic = model.state_dict()

          # Every 10 epochs, save the best mAP and best model as W&B Artifact  
          if (epoch+1)%2 == 0: 
            wandb.log({'best_mAP':best_mAP})

          print('\n')

    # Save the best model as W&B Artifact  
    wandb.log({'best_mAP':best_mAP})
    PATH = "/mnt/homeGPU/jgallego/wandb_artifacts/{}.pt".format(wandb.run.name)
    torch.save(best_model_state_dic, PATH)

    artifact = wandb.Artifact(name='{}'.format(run.name), type='model')
    artifact.add_file('/mnt/homeGPU/jgallego/wandb_artifacts/{}.pt'.format(run.name))
    run.log_artifact(artifact)

    # Evaluate best state model on the testing set
    test_loss, test_mAP, test_mAP50, test_mAP75, test_f1, test_precision, test_recall = validate_one_epoch(testloader, model, best_thr)
    wandb.log({'test_mAP':test_mAP, 'test_mAP50':test_mAP50, 'test_mAP75':test_mAP75, 'test_F1':test_f1, 'test_precision':test_precision, 'test_recall': test_recall})
    run.finish()

"""### W&B Sweeps"""

# Commented out IPython magic to ensure Python compatibility.

gc_collect()
if NEW_SWEEP: sweep_id = None
else: sweep_id = 'y0ac21kp'

if sweep_id == None:
    # Define the sweep configuration
    sweep_id = wandb.sweep(sweep={
            'method': 'bayes',
            'name': 'IHS4 + PCA Reduction + MaskRCNN',
            'metric': {'goal': 'maximize', 'name': 'val_mAP'},
            'parameters':
                {   
                    # Preprocessing and Dataset Config
                    # Parece que los mejores para índices de vegetación son: Brovey y Simple Mean
                    'pansharpening': {'values': ['IHS4']},
                    'dimensionality_reduction': {'values': ['PCA']},
                    'scaling': {'values' : [True, False]},
                    'drop_little_objets': {'values': [True, False]},
                    'drop_objects_area': {'values': [i**2 for i in range(3,15)]}, 

                    # Model config
                    'backbone': {'values': ['resnet50-fpn','resnet50-fpn-v2','resnet101'] + ['efficientnet_b{}'.format(i) for i in [0,2]] + ['mobilenetv2_100','resnext101_32x8d']},

                    # Training config
                    'batch_size' : {'values': [2, 4, 8, 16]}, 
                    'epochs': {'values': [200]},
                    'freezing': {'values': [0, 0.25, 0.5, 0.75]}, 
                    'image_size': {'values': [(99,80)]},
                    'dataset_split': {'values': [(0.8, 0.2)]},
                    
                    # Optimizer configuration.  
                    'optimizer': {'values': ['Adam', 'AdamW']},
                    'lr': {'values': [5e-05, 7.5e-05, 1e-04, 2.5e-04, 5e-04, 7.5e-04, 1e-03, 2.5e-03, 5e-03]}, 
                    'weight_decay': {'distribution': 'log_uniform_values', "min": 1e-03, "max": 0.1},
                    'amsgrad': {'values': [True, False]},
                 
                    # Lr Scheduler
                    'scheduler': {'values': ['ReduceLROnPlateau']},
                    'reducing_factor': {'values': [0.1, 0.25, 0.5, 0.75]},
                    'threshold': {'values': [0.001, 0.0025, 0.005]},

                    # Metric 
                    'mAP_type': {'values': ['bbox']},
                    
                }
        }, project="Bachelor Thesis")

    print('Generated sweep id', sweep_id)

else:
    """
    Agent run. Use sweep_id generated above to produce (semi)-random hyperparameters run.config
    """
    if __name__ == '__main__':

       print('=' * 30)
       print(' ' *12 + 'Training')
       print('=' * 30)

       for i in range(50):
        print('\n\n' + '='*25)
        print(i)
        print('='*25 + '\n\n')
        p_images, m_images, p_masks, m_masks = [], [], [], []
        fit_all_images(polygon_numbers)
        wandb.agent(sweep_id, function=train_model, entity="javigallego4", project="Bachelor Thesis", count = 1)

