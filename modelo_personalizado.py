# -*- coding: utf-8 -*-
"""Modelo Personalizado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rsYUkazGiYqMFWV0KhsbR43ZZuAU4BkT
"""

NEW_SWEEP = False

from IPython.display import clear_output, display_html
import gc; gc.enable()
import warnings
import os
from pathlib import Path
from tqdm import tqdm

# Basic libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import scipy as sc
from scipy import stats
import random
import cv2

# Preprocessing libraries
from sklearn.preprocessing import *
import cv2
import albumentations as A

# Library for .tiff files
import tifffile as tiff

# Timm Library
import timm

# PyTorch 
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, random_split
import torchvision
from torchvision import transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau 
from PIL import Image

# MaskRCNN class imports
from typing import Any, Callable, Optional
from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor
from torch import nn
from torchvision.ops import MultiScaleRoIAlign
from torchvision.ops import misc as misc_nn_ops
from torchvision.transforms._presets import ObjectDetection

from torchvision.models.detection.mask_rcnn import MaskRCNN, MaskRCNNPredictor
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.models import *
from torchvision.models.detection.mask_rcnn import _resnet_fpn_extractor

# Deep Lab V3 Backbones
from torchvision.models.segmentation.deeplabv3 import *
from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101, deeplabv3_mobilenet_v3_large, DeepLabV3_ResNet101_Weights

# Metric (mAP)
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# Weights and biases 
import wandb

# Memory usage
import gc
def gc_collect():
    gc.collect()
    torch.cuda.empty_cache()

warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

clear_output()
print('Number of CPUs: ', os.cpu_count())

DEBUG = False
LOG_IMAGES = False
#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
device = torch.device('cuda')
print(device)
train_validation_test_split = pd.read_csv('train_validation_test_split.csv', index_col = False)

"""Nos creamos los índices para los conjuntos de entrenamiento, validación y test."""

X_train = train_validation_test_split[train_validation_test_split['set'] == 'train'].image.values
X_test = train_validation_test_split[train_validation_test_split['set'] == 'test'].image.values
X_val = train_validation_test_split[train_validation_test_split['set'] == 'val'].image.values

"""# Preprocessing Pipeline"""

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

# Obtenemos todos los nºs de polígonos que nos dan en la siguiente ruta. 

BASE_PATH = "WV3/"
polygon_numbers = os.listdir(BASE_PATH)
polygon_numbers = pd.Series(polygon_numbers).str.split('_', n = 2, expand = True)[1]
polygon_numbers = list(polygon_numbers)
polygon_numbers = sorted(polygon_numbers)
polygon_numbers.remove('Store')

# Guardamos en arrays cada una de las imágenes y máscaras.

def load_images(polygon_number):
  # Panchromatic Images
  p_images.append(tiff.imread(BASE_PATH + "polygon_{}/panchromatic.tif".format(polygon_number)))
  p_masks.append(tiff.imread(BASE_PATH + "polygon_{}/mask_panchromatic.tif".format(polygon_number)))
  
  # Multispectral Images
  # Hacemos un permute para poner las imágenes en el formato PyTorch
  m_images.append(tiff.imread(BASE_PATH + "polygon_{}/multispectral.tif".format(polygon_number)))
  m_masks.append(tiff.imread(BASE_PATH + "polygon_{}/mask_multispectral.tif".format(polygon_number)))

  return p_images, m_images, p_masks, m_masks

def fit_all_images(polygon_numbers):
  with ThreadPoolExecutor(1) as p:
      for i in tqdm(p.map(load_images, polygon_numbers), total=len(polygon_numbers)):
          pass

"""## Pansharpening"""

def inverse_pca_image(sharpened_img, pca):
  """
    Performs inverse PCA transformation to the given image.

    Parameters:
    - sharpened_img: image to be transformed
    - pca: PCA object with loadings from previous fit_transform()
  """

  X = sharpened_img.permute(1,2,0).numpy()

  b0 = X[:,:,0]
  b1 = X[:,:,1]
  b2 = X[:,:,2]
  b3 = X[:,:,3]
  b4 = X[:,:,4]
  b5 = X[:,:,5]
  b6 = X[:,:,6]
  b7 = X[:,:,7]

  pca_df = pd.DataFrame({'B0': b0.flatten(), 'B1': b1.flatten(), 'B2': b2.flatten(), 'B3':b3.flatten(), 
              'B4': b4.flatten(), 'B5': b5.flatten(), 'B6': b6.flatten(), 'B7':b7.flatten()})

  img = pca.inverse_transform(pca_df)

  for i in range(8): 
    sharpened_img[i, :, :] = torch.from_numpy(img[:,i].reshape((sharpened_img.shape[1], sharpened_img.shape[2])))

  return sharpened_img

def histogram_match(pan, band):
    """
    Performs histogram matching between the panchromatic image and the multispectral band given.

    Parameters:
    - pan: torch tensor of shape (height, width)
    - band: torch tensor of shape (height, width)

    Returns:
    - matched_panchromatic: histogram matched PAN imagery
    """

    # Fórmula UGR: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwicxYCXrYv9AhXRhqQKHYoUDFQQFnoECAkQAQ&url=https%3A%2F%2Fccia.ugr.es%2Fvip%2Ffiles%2Fbooks%2Fpaper_amro_mateos.pdf&usg=AOvVaw3wn01QiErGCJLtNZUg-oKe
    matched_panchromatic = (pan - pan.mean()) * (band.std() / pan.std()) + band.mean()

    return matched_panchromatic

def pansharpen_image(multispectral_image, panchromatic_image, method):
    """
    Pansharpens the given MS image based on different techniques.

    Parameters:
    - multispectral_image: torch tensor of shape (channels, height, width)
    - panchromatic_image: torch tensor of shape (height, width)
    - method: type of pansharpening technique

    Returns:
    - sharpened_img: torch tensor with same shape as input multispectral image
    """
    
    if method == 'PCA': 
      # Forward Transform: perform PCA and replace 1st component with panchromatic band
      sharpened_img, pca = pca_image(multispectral_image, 8, True)
      sharpened_img = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(sharpened_img)

      # Histogram Matching: the PAN Imagery is matched with the PC1 band
      matched_panchromatic = histogram_match(panchromatic_image, sharpened_img[0, :, :])

      # Component Substitution
      sharpened_img[0, :, :] = matched_panchromatic

      # Reverse Transform: undo PCA
      sharpened_img = inverse_pca_image(sharpened_img, pca)
      
    if method == 'Simple Mean':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])
      for i in range(multispectral_image.shape[0]):
        # Histogram Matching for each band
        matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])        
        sharpened_img[i, :, :] = 0.5 * (multispectral_image[i, :, :] + matched_panchromatic)

    if method == 'Brovey': 
        multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
        sharpened_img = torch.randn(multispectral_image.shape[0], multispectral_image.shape[1], multispectral_image.shape[2])

        M = 0
        for i in range(multispectral_image.shape[0]):
          M += multispectral_image[i, :, :]
        M /= multispectral_image.shape[0]

        for i in range(multispectral_image.shape[0]):
          # Histogram Matching for each band
          matched_panchromatic = histogram_match(panchromatic_image, multispectral_image[i, :, :])
          sharpened_img[i, :, :] = (matched_panchromatic / M) * multispectral_image[i, :, :]

    if method == 'HSV': 
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 0.577 * (red+green+blue)
      v1 = -0.408 * (red+green) + 0.816 * blue
      v2 = -0.707 * (red+green) + 1.703 * blue

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 0.577 * matched_panchromatic - 0.408 * v1 - 0.707 * v2
      new_green = 0.577 * matched_panchromatic - 0.408 * v1 - 0.816 * v2
      new_blue = 0.577 * matched_panchromatic - 0.816 * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS1':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/np.sqrt(3) * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * (-red+green)

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      v1 = S * torch.cos(H)
      v2 = S * torch.sin(H)

      new_red = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 - 1/np.sqrt(2) * v2
      new_green = 1/np.sqrt(3) * matched_panchromatic -1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_blue = 1/np.sqrt(3) * matched_panchromatic + 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS2':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(6) * red - 2 /np.sqrt(6) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      v1 = S * torch.cos(2*np.pi*H)
      v2 = S * torch.sin(2*np.pi*H)

      new_red = 1 * matched_panchromatic -0.204124 * v1 - 0.612372 * v2
      new_green = 1 * matched_panchromatic -0.204124 * v1 + 0.612372 * v2
      new_blue = 1 * matched_panchromatic + 0.408248 * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS3':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -1/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(6) * red - 1/np.sqrt(6) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic -1/np.sqrt(6) * v1 +3/np.sqrt(6) * v2
      new_green = 1 * matched_panchromatic -1/np.sqrt(6) * v1 -3/np.sqrt(6) * v2
      new_blue = 1 * matched_panchromatic + 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS4':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v1/v2)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1/3 * matched_panchromatic +1/np.sqrt(6) * v1 -1/np.sqrt(2) * v2
      new_blue = 1/3 * matched_panchromatic - 1/np.sqrt(2) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS5':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'IHS6':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = -2/np.sqrt(6) * (red+green) + 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v2/v1)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic -1/np.sqrt(2) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic -1/np.sqrt(2) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic + np.sqrt(2) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    if method == 'HLS':
      multispectral_image = torchvision.transforms.Resize((panchromatic_image.shape[0], panchromatic_image.shape[1]))(multispectral_image)
      # Forward Transform
      red = multispectral_image[4, :, :]
      green = multispectral_image[2, :, :]
      blue = multispectral_image[1, :, :]

      I = 1/3 * (red+green+blue)
      v1 = 1/np.sqrt(6) * (red+green) - 2/np.sqrt(6) * blue
      v2 = 1/np.sqrt(2) * red - 1/np.sqrt(2) * green

      H = torch.atan(v1/v2)
      S = torch.sqrt(torch.pow(v1,2) + torch.pow(v2,2))

      # Histogram Matching
      matched_panchromatic = histogram_match(panchromatic_image, I)

      # Reverse Transformation
      new_red = 1 * matched_panchromatic +1/np.sqrt(6) * v1 + 1/np.sqrt(2) * v2
      new_green = 1 * matched_panchromatic +1/np.sqrt(6) * v1  -1/np.sqrt(2) * v2
      new_blue = 1 * matched_panchromatic - 2/np.sqrt(6) * v1

      sharpened_img = multispectral_image
      sharpened_img[4, :, :] = new_red
      sharpened_img[2, :, :] = new_green
      sharpened_img[1, :, :] = new_blue

    return sharpened_img

def pansharpening(method):
  """
  Performs pansharpening to every image in the dataset.
  """

  for i in range(len(p_images)):
    img = pansharpen_image(torch.from_numpy(m_images[i]).permute(2,0,1), torch.from_numpy(p_images[i]), method)
    m_images[i] = img.permute(1,2,0).numpy()

if DEBUG: 
  #scaling()
  for i in range(2):

    # Vemos como sería la imagen multispectral con la resolucion de la pancromática.
    multispectral_image = torchvision.transforms.Resize((p_images[i].shape[0], p_images[i].shape[1]))(torch.from_numpy(m_images[i].astype(np.int)).permute(2,0,1))
    plt.imshow(multispectral_image[:3, :, :].permute(1,2,0))
    plt.show()

    # Aplicamos el pansharpening para observar el cambio.
    img = pansharpen_image(torch.from_numpy(m_images[i].astype(np.float)).permute(2,0,1), torch.from_numpy(p_images[i].astype(np.float)), 'IHS3')
    fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (12, 8))

    axes[0,0].imshow(img[:3, :, :].permute(1,2,0))
    axes[0,1].imshow(img[:3, :, :].permute(1,2,0))
    axes[0,1].imshow(p_masks[i], alpha = .5)

    axes[1,0].imshow(p_images[i], cmap = 'gray')
    axes[1,1].imshow(p_images[i])
    axes[1,1].imshow(p_masks[i], alpha = .5)
    plt.show()

"""## Scaling"""

def scale_panchromatic_image(image, transformer = MinMaxScaler()):
  '''Returns input panchromatic image with its values being scaled to the [0,1] interval. '''

  img = transformer.fit_transform(image, )
  return img

def scale_multispectral_image(image, bands = 8, transformer = MinMaxScaler()):
  '''Returns input multispectral image with its values being scaled to the [0,1] interval. '''

  b0 = image[:,:,0]
  b1 = image[:,:,1]
  b2 = image[:,:,2]
  if bands == 8: 
    b3 = image[:,:,3]
    b4 = image[:,:,4]
    b5 = image[:,:,5]
    b6 = image[:,:,6]
    b7 = image[:,:,7]

  # As before, we perform some scaling first
  sc = transformer
  b0 = sc.fit_transform(b0)
  b1 = sc.fit_transform(b1)
  b2 = sc.fit_transform(b2)
  if bands == 8: 
    b3 = sc.fit_transform(b3)
    b4 = sc.fit_transform(b4)
    b5 = sc.fit_transform(b5)
    b6 = sc.fit_transform(b6)
    b7 = sc.fit_transform(b7)

  if bands == 8: img = np.dstack([b0, b1, b2, b3, b4, b5, b6, b7])
  else: img = np.dstack([b0, b1, b2])
  return img

def scaling():
  '''Pipeline function for value scaling. '''

  for i in range(len(p_images)):
    p_images[i] = scale_panchromatic_image(p_images[i])
    m_images[i] = scale_multispectral_image(m_images[i])

"""## Data Augmentation"""

def HorizontalFlip():
  '''Performs horizontal flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], 1))
    p_masks.append(cv2.flip(p_masks[i], 1))

def VerticalFlip():
  '''Performs vertical flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], 0))
    p_masks.append(cv2.flip(p_masks[i], 0))

def VHFlip(): 
  '''Performs both horizontal and vertical flipping. '''

  for i in range(len(polygon_numbers)):
    m_images.append(cv2.flip(m_images[i], -1))
    p_masks.append(cv2.flip(p_masks[i], -1))

def Rotation90():
  '''Performs a 90 degrees rotation on the images'''

  for i in range(len(polygon_numbers)):

    # Transpose the image
    image = image.transpose(1,0,2)
    # Flip the image vertically
    image = cv2.flip(image, 1)
    m_images.append(cv2.flip(m_images[i], -1))
    p_masks.append(cv2.flip(p_masks[i], -1))
  
# source: https://www.kaggle.com/safavieh/image-augmentation-using-skimage
import random
import pylab as pl 

def random_crop(img, mask, crop_height, crop_width):
    
    height, width = img.shape[0], img.shape[1]

    # Calculate aspect ratio
    aspect_ratio = float(width / height)

    # Calculate the maximum width and height that can be cropped while maintaining the aspect ratio
    max_crop_width = int(aspect_ratio * crop_height)
    max_crop_height = int(crop_width / aspect_ratio)

    # Choose a random starting point for the crop
    start = random.randint(0,10)
    x = random.randint(start, width - max_crop_width)
    y = random.randint(start, height - max_crop_height)

    # Crop the image and mask
    cropped_img = img[y:y+max_crop_height, x:x+max_crop_width]
    cropped_mask = mask[y:y+max_crop_height, x:x+max_crop_width]

    # Resize the cropped image and mask to the desired size
    # Interpolation. Possible values: cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_AREA, cv2.INTER_CUBIC
    resized_image = cv2.resize(cropped_img, (width, height), interpolation=cv2.INTER_LINEAR)
    resized_mask = cv2.resize(cropped_mask, (width, height), interpolation=cv2.INTER_LINEAR)

    return resized_image, resized_mask

def RandomCropping(): 
    ''' Applying 10 random croppings to all the images.'''

    for i in range(len(polygon_numbers)):
      for j in range(10):
          width = random.randint(40, 60)
          aspect_ratio = m_images[i].shape[0] / m_images[i].shape[1]
          img, mask = random_crop(m_images[i], p_masks[i], int(width*aspect_ratio), width)
          m_images.append(img)
          p_masks.append(mask)

def data_augmentation():
  '''Performs data augmentation over images and masks. '''

  HorizontalFlip()
  VerticalFlip()
  VHFlip()
  RandomCropping()
  #GaussianBlur()

def visualize(image, mask, original_image=None, original_mask=None):
    fontsize = 18
    
    if original_image is None and original_mask is None:
        f, ax = plt.subplots(2, 1, figsize=(8, 8))

        ax[0].imshow(image)
        ax[1].imshow(mask)
    else:
        f, ax = plt.subplots(2, 2, figsize=(8, 8))

        ax[0, 0].imshow(original_image)
        ax[0, 0].set_title('Original image', fontsize=fontsize)
        
        ax[1, 0].imshow(original_mask)
        ax[1, 0].set_title('Original mask', fontsize=fontsize)
        
        ax[0, 1].imshow(image)
        ax[0, 1].set_title('Transformed image', fontsize=fontsize)
        
        ax[1, 1].imshow(mask)
        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)

        plt.show()

"""## Padding"""

def pad_images(imgs, msks, border):
  '''Pipeline function for images' padding. '''

  border_type = border
  images, masks = [], []
  for i in range(len(imgs)):
    images.append(cv2.copyMakeBorder(imgs[i], 128 - imgs[i].shape[0], 0, 80 - imgs[i].shape[1], 0, border_type))
    masks.append(cv2.copyMakeBorder(msks[i], 128 - msks[i].shape[0], 0, 80 - msks[i].shape[1], 0, border_type))

  return images, masks

"""## Preprocessing Pipeline Main Function"""

def preprocessing_pipeline(method, border_type = cv2.BORDER_CONSTANT, scale = True): 
  for i in range(len(m_images)):
    m_images[i] = m_images[i].astype(float)
    p_images[i] = p_images[i].astype(float)
  
  if scale: scaling()
  pansharpening(method)
  data_augmentation()
  imgs, masks = pad_images(m_images, p_masks, border_type)

  for i in range(len(imgs)):
    masks[i] = masks[i].astype(int)

  return imgs, masks

#m_images, p_masks = preprocessing_pipeline('Simple Mean')

"""# PyTorch Dataset"""

class PanchromaticDataset(torch.utils.data.Dataset):
    def __init__(self, images, masks):
      super().__init__()
      self.images = images 
      self.masks = masks 

    def __getitem__(self, i):
      return torch.from_numpy(self.images[i].astype(float)), torch.from_numpy(self.masks[i].astype(float))
        
    def __len__(self):
      return len(self.images)

class MultispectralDataset(torch.utils.data.Dataset):
  def __init__(self, images, masks):
    super().__init__()
    self.images = images 
    self.masks = masks 

  def __getitem__(self, i):
    img, mask = torch.from_numpy(self.images[i].astype(float)).permute(2,0,1), torch.from_numpy(self.masks[i].astype(float))
    return torch.nan_to_num(img), torch.nan_to_num(mask)

  def __len__(self):
      return len(self.images)

"""# Model

### Model Implementation
"""

from torchvision.models import *
from torch.nn.modules import batchnorm

class ConvBlock(nn.Module): 
    ''' Basic block for performing two convolution operations. '''

    def __init__(self, in_channels, out_channels, pad = 1, activation_function = nn.ReLU(inplace=True)): 
        super(ConvBlock, self).__init__()
        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride = 1, padding=pad)
        batch1 = nn.BatchNorm2d(out_channels)

        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride = 1, padding=pad)
        batch2 = nn.BatchNorm2d(out_channels)

        self.conv_block = nn.Sequential(conv1, batch1, activation_function, conv2, batch2, activation_function)

    def forward(self, x): 
        x = self.conv_block(x)
        return x

class unetDown(nn.Module):
    ''' Encoder block of the U-Net architecture. '''

    def __init__(self, in_channels, out_channels, pad = 1):
        super(unetDown, self).__init__()
        self.conv_block = ConvBlock(in_channels, out_channels, pad)
        self.pooling = nn.MaxPool2d(kernel_size = 2, stride = 2)

    def forward(self, x, indices = None): 
        skip_connection = self.conv_block(x)
        x = self.pooling(skip_connection)
        return x, skip_connection

class unetUp(nn.Module):
    ''' Decoder block of the U-Net architecture. '''

    def __init__(self, in_channels, out_channels, pad=1):
        super(unetUp, self).__init__()
        self.upsample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)
        self.conv_block = ConvBlock(in_channels, out_channels, pad)

    def forward(self, x, skip_connection): 
        x = self.upsample(x)
        #print('nnnn')
        #print(x.shape)
        #print(skip_connection.shape)
        x = torch.cat([x, skip_connection], dim=1)
        x = self.conv_block(x)
        return x

class multispectralUnet(nn.Module):
    ''' Full architecture of the proposed network. '''

    def __init__(self, n_filters_1, n_filters_2, n_filters_3, n_filters_4, conv_init):
        super(multispectralUnet, self).__init__()

        # ====== Encoder ======

        self.down1 = unetDown(8, n_filters_1)
        self.down2 = unetDown(n_filters_1, n_filters_2)
        self.down3 = unetDown(n_filters_2, n_filters_3)

        # ====== BottleNeck =====
        self.bottleneck = ConvBlock(n_filters_3, n_filters_4)

        # ===== Decoder =====

        self.up3 = unetUp(n_filters_3 + n_filters_4, n_filters_3)
        self.up2 = unetUp(n_filters_2 + n_filters_3, n_filters_2)
        self.up1 = unetUp(n_filters_1 + n_filters_2, n_filters_1)
        
        # Final Convolution
        self.conv_last = nn.Sequential(nn.Conv2d(n_filters_1, 1, kernel_size=1), nn.Tanh())

        self._init_weights(self.down1, conv_init)
        self._init_weights(self.down2, conv_init)
        self._init_weights(self.down3, conv_init)
        self._init_weights(self.up1, conv_init)
        self._init_weights(self.up2, conv_init)
        self._init_weights(self.up3, conv_init)
        self._init_weights(self.conv_last, conv_init)

    def forward(self, x):
        x, skip1 = self.down1(x)
        x, skip2 = self.down2(x)
        x, skip3 = self.down3(x)

        x = self.bottleneck(x)

        x = self.up3(x, skip3)
        x = self.up2(x, skip2)
        x = self.up1(x, skip1)
        x = self.conv_last(x)
        return x

    def predict(self,x):
        x = self.forward(x)
        return x

    def _init_weights(self, module, conv_init = 'normal'):
        if isinstance(module, nn.Conv2d):
            if conv_init == 'normal': nn.init.normal_(module.weight.data, mean=0.0, std=0.02)
            elif conv_init == 'uniform': nn.init.uniform_(module.weight.data, 0.0, 0.02)
            elif conv_init == 'xavier': nn.init.xavier_normal_(module.weight.data, gain=1)
            elif conv_init == 'kaiming': nn.init.kaiming_normal_(module.weight.data, a=0, mode='fan_in')
            if module.bias is not None: nn.init.zeros_(module.bias.data)

"""# Training Pipeline

### Helper Functions
"""

def optimal_f1(predictions, targets):
    '''With this function we obtain the optimal threshold, given our model predictions. '''
    
    thres = np.linspace(torch.min(predictions).item(), torch.max(predictions).item(), 201)
    f1_score = F1Score(task="binary", num_classes=2).to(device)
    f1s = torch.Tensor([f1_score(predictions > thr, targets) for thr in thres])
    idx = torch.argmax(f1s)
    return f1s[idx], thres[idx]

class EarlyStopping():
    """
    Early stopping to stop the training when the loss does not improve after
    certain epochs.
    """
    def __init__(self, patience=5, min_delta=0):
        """
        :param patience: how many epochs to wait before stopping when loss is
               not improving
        :param min_delta: minimum difference between new loss and old loss for
               new loss to be considered as an improvement
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        
    def __call__(self, val_loss, val_mAP):
        if val_mAP <= 0.001: 
            print('Very low F1 Score')
            print('INFO: Early stopping')
            self.early_stop = True

        if self.best_loss == None:
            self.best_loss = val_loss
            
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            # reset counter if validation loss improves
            self.counter = 0
            
        elif self.best_loss - val_loss <= self.min_delta:
            self.counter += 1
            print(f"INFO: Early stopping counter {self.counter} of {self.patience}")
            if self.counter >= self.patience:
                print('INFO: Early stopping')
                self.early_stop = True

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True

from torchmetrics import F1Score, Precision, Recall
#from sklearn.metrics import f1_score, precision_score, recall_score

def train_one_epoch(train_loader, model, optimizer, scaler):
    # Track losses
    loss_epoch = 0
    f1_epoch, precision_epoch, recall_epoch = 0.0, 0.0, 0.0
    bce_loss = nn.BCEWithLogitsLoss().to(device)
    #f1_score = F1Score(task="binary", num_classes=2).to(device)
    precision_score = Precision(task="binary", num_classes=2).to(device)
    recall_score = Recall(task="binary", num_classes=2).to(device)
    thr_list = []

    # Loop over minibatches
    for imgs, masks in tqdm(train_loader):
        model.train() # Train mode

        imgs = imgs.to(device, dtype=torch.float32)
        masks = masks.to(device)

        # Zero gradients
        optimizer.zero_grad(set_to_none=True)

        # Forward pass
        with torch.cuda.amp.autocast():
          outputs = model(imgs)
          loss = bce_loss(outputs.squeeze()[:,29:,:], masks[:,29:,:])

        # Scales the loss, and calls backward() to create scaled gradients
        scaler.scale(loss).backward()

        # Unscales gradients and calls or skips optimizer.step()
        scaler.step(optimizer)

        # Updates the scale for next iteration
        scaler.update()
            
        # Track losses
        loss_epoch += loss.detach().item()

        # Track metric
        # Don't update weights
        with torch.no_grad():
          model.eval()
          preds = model.predict(imgs).squeeze()

          for i, mask in enumerate(preds): 
              f1_output, threshold = optimal_f1(mask[29:, :], masks[i].squeeze()[29:, :]) 
              thr_list.append(threshold.item())
              f1_epoch += f1_output 
              recall_epoch += recall_score(mask[29:, :] > threshold, masks[i].squeeze()[29:, :]) 
              precision_epoch += precision_score(mask[29:, :] > threshold, masks[i].squeeze()[29:, :]) 
        
    return loss_epoch/len(train_loader), f1_epoch/len(train_loader), precision_epoch/len(train_loader), recall_epoch/len(train_loader), sum(thr_list)/len(thr_list), model

def validate_one_epoch(validation_loader, model, thr):
    #metric = MeanAveragePrecision(iou_type = 'bbox')
    loss_epoch = 0
    bce_loss = nn.BCEWithLogitsLoss().to(device)
    f1_epoch, precision_epoch, recall_epoch = 0.0, 0.0, 0.0
    f1_score = F1Score(task="binary", num_classes=2).to(device)
    precision_score = Precision(task="binary", num_classes=2).to(device)
    recall_score = Recall(task="binary", num_classes=2).to(device)
    
    # Don't update weights
    with torch.no_grad():

      # Loop over minibatches
      for imgs, masks in tqdm(validation_loader):
          model.train()
          imgs = imgs.to(device, dtype=torch.float32)
          masks = masks.to(device)

          # Make predictions and obtain losses
          with torch.cuda.amp.autocast():
            outputs = model(imgs)
            loss = bce_loss(outputs.squeeze()[:, 29:, :], (masks[:, 29:, :]).to(device))
  
          # Track losses
          loss_epoch += loss.detach().item()
          
          # Track metric
          model.eval()
          preds = model.predict(imgs).squeeze()

          f1_epoch += f1_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :]) 
          recall_epoch += recall_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :]) 
          precision_epoch += precision_score(preds[:, 29:, :] > thr, masks.squeeze()[:, 29:, :]) 

    return loss_epoch/len(validation_loader), f1_epoch/len(validation_loader), precision_epoch/len(validation_loader), recall_epoch/len(validation_loader)

def add_weight_decay(model, weight_decay=1e-5, skip_list=()):
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):
            # print(name, 'no decay')
            no_decay.append(param)
        else:
            # print(name, 'decay')
            decay.append(param)
    return [
        {'params': no_decay, 'weight_decay': 0.},
        {'params': decay, 'weight_decay': weight_decay}]

print('WANDB Logging')
if NEW_SWEEP == False: os.environ['WANDB_DIR'] = '/mnt/homeGPU/jgallego/'
if NEW_SWEEP == False: os.environ['WANDB_CONFIG_DIR'] = '/mnt/homeGPU/jgallego/'
if NEW_SWEEP == False: os.environ['WANDB_CACHE_DIR'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'
if NEW_SWEEP == False: os.environ['WANDB_ARTIFACT_STAGING'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'
if NEW_SWEEP == False: os.environ['WANDB_DATA_DIR'] = '/mnt/homeGPU/jgallego/wandb_artifacts/'

os.environ["WANDB_API_KEY"] = '5bf911e7e682da23240c68fb146a222bf0475f7c'
wandb.login() # 5bf911e7e682da23240c68fb146a222bf0475f7c
print('Logging DONE\n')

"""### Main Function"""

def train_model(verbose=True):
    torch.manual_seed(42)
    # Init W&B 
    run = wandb.init(group = 'Custom Model - Tuning')
    if NEW_SWEEP == False: os.system('chmod -R 777 /mnt/homeGPU/jgallego/')

    # Preprocessing Pipeline (No initial scaling)
    m_images2, p_masks2 = preprocessing_pipeline(wandb.config.pansharpening, cv2.BORDER_CONSTANT, wandb.config.scaling)
    print('Preprocessing done')

    # Datasets and train-test-split
    multispectral_dataset = MultispectralDataset(m_images2, p_masks2)
    n = len(multispectral_dataset)
    training_data = torch.utils.data.Subset(multispectral_dataset, X_train)
    validation_data = torch.utils.data.Subset(multispectral_dataset, X_val)
    test_data = torch.utils.data.Subset(multispectral_dataset, X_test)
    gc_collect()

    print('Dataset created')

    # Model
    model = multispectralUnet(wandb.config.n_filters_1, wandb.config.n_filters_2, wandb.config.n_filters_3, wandb.config.n_filters_4, wandb.config.init_conv).to(device)
    print('Model created')
    
    params = add_weight_decay(model, weight_decay=wandb.config.weight_decay, skip_list=['bias'])
    if wandb.config.optimizer == 'AdamW': 
      optimizer = optim.AdamW(params, lr = wandb.config.lr, weight_decay = wandb.config.weight_decay, amsgrad = wandb.config.amsgrad) 
    elif wandb.config.optimizer == 'Adam': 
      optimizer = optim.Adam(params, lr = wandb.config.lr, weight_decay = wandb.config.weight_decay, amsgrad = wandb.config.amsgrad) 
    
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=wandb.config.reducing_factor, patience=3, threshold = wandb.config.threshold, verbose = True)

    # Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.
    scaler = torch.cuda.amp.GradScaler()
    
    #for i in range(wandb.config.folds): 
    #   train_idx = train[train.VAL_FOLD != i].index
    #    val_idx = train[train.VAL_FOLD == i].index
    #batch = wandb.config.batch_size if wandb.config.backbone == 'ResNet-50-FPN' else int(wandb.config.batch_size / 2)     
    batch = wandb.config.batch_size
    trainloader = torch.utils.data.DataLoader(training_data, batch_size = batch, num_workers = 40, shuffle = True, pin_memory=True)
    validationloader = torch.utils.data.DataLoader(validation_data, batch_size = batch, num_workers = 40, pin_memory=True)
    testloader = torch.utils.data.DataLoader(test_data, batch_size = batch, num_workers = 40, pin_memory=True)

        #print('====== Fold: {} ======='.format(i))
    
    best_f1 = 0
    best_thr = 0.5
    early_stopping = EarlyStopping(10, wandb.config.threshold)

    # Loop over epochs
    for epoch in range(200):
            
          # Train
          train_loss, train_f1, train_precision, train_recall, thr, model = train_one_epoch(trainloader, model, optimizer, scaler)
          
          # Evaluate
          val_loss, val_f1, val_precision, val_recall = validate_one_epoch(validationloader, model, thr)

          # Apply scheduler
          scheduler.step(val_loss)
          
          # Log metrics
          wandb.log({
              'epoch': epoch,
              'train_loss': train_loss,
              'val_loss': val_loss,
              'train_f1': train_f1, 
              'train_precision': train_precision, 
              'train_recall': train_recall,
              'val_f1': val_f1, 
              'val_precision': val_precision, 
              'val_recall': val_recall, 
              'threshold': thr
          })

          # Print loss
          if verbose:
              if (epoch+1)%1==0:
                  print(f'\nEpoch {epoch+1}/20')
                  print(f'BCE Loss {train_loss:.5f}, VAL BCE Loss {val_loss:.5f}')
                  print(f'f1 {train_f1:.5f}, precision {train_precision:.5f}, recall {train_recall:.5f}, val_f1 {val_f1:.5f}, val_precision {val_precision:.5f}, val_recall {val_recall:.5f}')  
                  print(f'Threshold {thr}')
  
          # Early Stopping
          early_stopping(val_loss, val_f1)
          if early_stopping.early_stop:
              break
          else:
              if val_f1 > best_f1: 
                  best_f1 = val_f1
                  best_thr = thr
                  best_model_state_dic = model.state_dict()

          # Every 10 epochs, save the best mAP and best model as W&B Artifact  
          if (epoch+1)%2 == 0: 
            wandb.log({'best_f1':best_f1})

          print('\n')

  
    # Save the best model as W&B Artifact  
    wandb.log({'best_f1':best_f1})
    wandb.log({'best_thr':best_thr})
    PATH = "/mnt/homeGPU/jgallego/wandb_artifacts/{}.pt".format(wandb.run.name)
    torch.save(best_model_state_dic, PATH)
    artifact = wandb.Artifact(name='{}'.format(run.name), type='model')
    artifact.add_file('/mnt/homeGPU/jgallego/wandb_artifacts/{}.pt'.format(run.name))
    run.log_artifact(artifact)

    # Log test metrics and finish run
    test_loss, test_f1, test_precision, test_recall = validate_one_epoch(testloader, model, best_thr)
    wandb.log({'test_F1':test_f1, 'test_precision':test_precision, 'test_recall': test_recall})
    run.finish()

gc_collect()
if NEW_SWEEP: sweep_id = None
else: sweep_id = 'vdb12b14'

if sweep_id == None:
    # Define the sweep configuration
    sweep_id = wandb.sweep(sweep={
            'method': 'bayes',
            'name': 'Simple Mean + Vegetation Indexes Reduction + Custom Unet (3)',
            'metric': {'goal': 'maximize', 'name': 'val_f1'},
            'parameters':
                {   
                    # Preprocessing and Dataset Config
                    'pansharpening': {'values': ['Simple Mean','IHS3','IHS5','HLS']},
                    'dimensionality_reduction': {'values': ['Vegetation Indexes']},
                    'scaling': {'values' : [True, False]},

                    # Model config
                    'init_conv': {'values': ['normal','kaiming','xavier','uniform']}, 
                    'n_filters_1': {'values': [16,32,64,96,128,256,512]},
                    'n_filters_2': {'values': [16,32,64,96,128,256,512]},
                    'n_filters_3': {'values': [16,32,64,96,128,256,512]},
                    'n_filters_4': {'values': [16,32,64,96,128,256,512]},

                    # Training config
                    'batch_size' : {'values': [2, 4, 8, 16, 32, 64, 128, 256]}, 
                    'epochs': {'values': [200]},
                    'image_size': {'values': [(99,80)]},
                    'dataset_split': {'values': [(0.8, 0.2)]},
                    
                    # Optimizer configuration.  
                    'optimizer': {'values': ['Adam', 'AdamW']},
                    'lr': {'values': [5e-05, 7.5e-05, 1e-04, 2.5e-04, 5e-04, 7.5e-04, 1e-03, 2.5e-03, 5e-03, 7.5e-03, 1e-02]}, 
                    'weight_decay': {'distribution': 'log_uniform_values', "min": 1e-03, "max": 0.1},
                    'amsgrad': {'values': [True, False]},
                 
                    # Lr Scheduler
                    'scheduler': {'values': ['ReduceLROnPlateau']},
                    'reducing_factor': {'values': [0.1, 0.25, 0.5, 0.75]},
                    'threshold': {'values': [0.001, 0.0025, 0.005]},

                }
        }, project="Bachelor Thesis")

    print('Generated sweep id', sweep_id)

else:
    """
    Agent run. Use sweep_id generated above to produce (semi)-random hyperparameters run.config
    """
    if __name__ == '__main__':

      print('=' * 30)
      print(' ' *12 + 'Training')
      print('=' * 30)

      for i in range(50):
        print('\n\n' + '='*25)
        print(i)
        print('='*25 + '\n\n')
        p_images, m_images, p_masks, m_masks = [], [], [], []
        fit_all_images(polygon_numbers)
        wandb.agent(sweep_id, function=train_model, entity="javigallego4", project="Bachelor Thesis", count = 1)

        del p_images, m_images, p_masks, m_masks
        gc_collect()

